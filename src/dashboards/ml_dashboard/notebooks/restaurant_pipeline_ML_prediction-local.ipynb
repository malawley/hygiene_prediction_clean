{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43356,
     "status": "ok",
     "timestamp": 1745853174671,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "cKDBQVnvvRn0",
    "outputId": "4116d9b3-55b0-484d-c109-aa61cf8507c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Authenticated as: extractor-access@hygiene-prediction.iam.gserviceaccount.com\n",
      "âœ… Environment initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Local Jupyter Notebook Initialization ===\n",
    "# \n",
    "# This cell sets up the Jupyter environment for running cloud operations.\n",
    "# - Enables multiple outputs per cell\n",
    "# - Verifies the active GCP account and project are correctly set\n",
    "# - Initializes BigQuery and GCS clients\n",
    "# \n",
    "# IMPORTANT:\n",
    "# This notebook expects:\n",
    "# - GCP Account: malawley434@gmail.com\n",
    "# - GCP Project: hygiene-prediction-434\n",
    "# - Run this command in linux to check \"gcloud config list\"\n",
    "# If authentication is not correct, manually run `make switch-account` in root directory \n",
    "# C:\\Users\\malaw\\OneDrive\\Documents\\MSDS\\MSDS434\\hygiene_prediction \n",
    "# before starting Jupyter.\n",
    "\n",
    "# Enable multiple outputs per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Print authenticated identity\n",
    "from google.auth import default\n",
    "creds, _ = default()\n",
    "try:\n",
    "    print(f\"ðŸ” Authenticated as: {creds.service_account_email}\")\n",
    "except AttributeError:\n",
    "    print(\"ðŸ” Authenticated using user credentials (no service account email).\")\n",
    "\n",
    "print(\"âœ… Environment initialized successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2714,
     "status": "ok",
     "timestamp": 1745853187005,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "5CFTMnZk01Mp",
    "outputId": "0e7e9a6b-f569-4736-aa75-497a1dc5199a"
   },
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 POST https://bigquery.googleapis.com/bigquery/v2/projects/hygiene-prediction-434/jobs?prettyPrint=false: Access Denied: Project hygiene-prediction-434: User does not have bigquery.jobs.create permission in project hygiene-prediction-434.\n\nLocation: None\nJob ID: 374342d4-1c28-4992-90c3-a7411bb2b54f\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m\n\u001b[0;32m     14\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124mCREATE OR REPLACE TABLE `hygiene-prediction-434.RestaurantModeling.RestaurantProfile_WithFlags` AS\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124mSELECT\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m  `hygiene-prediction-434.RestaurantModeling.RestaurantProfile`\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Run the query\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m query_job \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m query_job\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… RestaurantProfile_WithFlags table created successfully with flags.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:3502\u001b[0m, in \u001b[0;36mClient.query\u001b[1;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[0;32m   3491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _job_helpers\u001b[38;5;241m.\u001b[39mquery_jobs_query(\n\u001b[0;32m   3492\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3493\u001b[0m         query,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3499\u001b[0m         job_retry,\n\u001b[0;32m   3500\u001b[0m     )\n\u001b[0;32m   3501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m api_method \u001b[38;5;241m==\u001b[39m enums\u001b[38;5;241m.\u001b[39mQueryApiMethod\u001b[38;5;241m.\u001b[39mINSERT:\n\u001b[1;32m-> 3502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_job_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_jobs_insert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3503\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_id_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3513\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected value for api_method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(api_method)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\cloud\\bigquery\\_job_helpers.py:181\u001b[0m, in \u001b[0;36mquery_jobs_insert\u001b[1;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_retry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     do_query \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mcloud\u001b[38;5;241m.\u001b[39mbigquery\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39m_DEFAULT_QUERY_JOB_INSERT_RETRY(do_query)\n\u001b[1;32m--> 181\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[43mdo_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# The future might be in a failed state now, but if it's\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# unrecoverable, we'll find out when we ask for it's result, at which\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# point, we may retry.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m job_id_given:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\cloud\\bigquery\\_job_helpers.py:137\u001b[0m, in \u001b[0;36mquery_jobs_insert.<locals>.do_query\u001b[1;34m()\u001b[0m\n\u001b[0;32m    134\u001b[0m query_job \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mQueryJob(job_ref, query, client\u001b[38;5;241m=\u001b[39mclient, job_config\u001b[38;5;241m=\u001b[39mjob_config)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core_exceptions\u001b[38;5;241m.\u001b[39mConflict \u001b[38;5;28;01mas\u001b[39;00m create_exc:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# The thought is if someone is providing their own job IDs and they get\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# their job ID generation wrong, this could end up returning results for\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# the wrong query. We thus only try to recover if job ID was not given.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m job_id_given:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1383\u001b[0m, in \u001b[0;36mQueryJob._begin\u001b[1;34m(self, client, retry, timeout)\u001b[0m\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"API call:  begin the job via a POST request\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m \n\u001b[0;32m   1365\u001b[0m \u001b[38;5;124;03mSee\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;124;03m    ValueError: If the job has already begun.\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1383\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mQueryJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mGoogleAPICallError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1385\u001b[0m     exc\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m _EXCEPTION_FOOTER_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1386\u001b[0m         message\u001b[38;5;241m=\u001b[39mexc\u001b[38;5;241m.\u001b[39mmessage, location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation, job_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_id\n\u001b[0;32m   1387\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\cloud\\bigquery\\job\\base.py:748\u001b[0m, in \u001b[0;36m_AsyncJob._begin\u001b[1;34m(self, client, retry, timeout)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;66;03m# jobs.insert is idempotent because we ensure that every new\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;66;03m# job has an ID.\u001b[39;00m\n\u001b[0;32m    747\u001b[0m span_attributes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}\n\u001b[1;32m--> 748\u001b[0m api_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspan_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBigQuery.job.begin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspan_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_api_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_properties(api_response)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:843\u001b[0m, in \u001b[0;36mClient._call_api\u001b[1;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[0;32m    841\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[0;32m    842\u001b[0m     ):\n\u001b[1;32m--> 843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hygiene-ml\\lib\\site-packages\\google\\cloud\\_http\\__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/hygiene-prediction-434/jobs?prettyPrint=false: Access Denied: Project hygiene-prediction-434: User does not have bigquery.jobs.create permission in project hygiene-prediction-434.\n\nLocation: None\nJob ID: 374342d4-1c28-4992-90c3-a7411bb2b54f\n"
     ]
    }
   ],
   "source": [
    "# === Create RestaurantProfile_WithFlags Table ===\n",
    "# This cell creates a new BigQuery table called 'RestaurantProfile_WithFlags'\n",
    "# by adding binary flag columns (is_cafe, is_bar, is_bakery, etc.)\n",
    "# based on the 'types' field from the original RestaurantProfile table.\n",
    "# These flags will later be used as features for machine learning models.\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=\"hygiene-prediction-434\")\n",
    "\n",
    "# SQL to create a new table with flags\n",
    "query = \"\"\"\n",
    "CREATE OR REPLACE TABLE `hygiene-prediction-434.RestaurantModeling.RestaurantProfile_WithFlags` AS\n",
    "SELECT\n",
    "  *,\n",
    "  CASE WHEN 'cafe' IN UNNEST(types) THEN 1 ELSE 0 END AS is_cafe,\n",
    "  CASE WHEN 'bar' IN UNNEST(types) THEN 1 ELSE 0 END AS is_bar,\n",
    "  CASE WHEN 'bakery' IN UNNEST(types) THEN 1 ELSE 0 END AS is_bakery,\n",
    "  CASE WHEN 'meal_takeaway' IN UNNEST(types) THEN 1 ELSE 0 END AS is_meal_takeaway,\n",
    "  CASE WHEN 'meal_delivery' IN UNNEST(types) THEN 1 ELSE 0 END AS is_meal_delivery,\n",
    "  CASE WHEN 'night_club' IN UNNEST(types) THEN 1 ELSE 0 END AS is_night_club\n",
    "FROM\n",
    "  `hygiene-prediction-434.RestaurantModeling.RestaurantProfile`\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "query_job = client.query(query)\n",
    "query_job.result()\n",
    "\n",
    "print(\"âœ… RestaurantProfile_WithFlags table created successfully with flags.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2966,
     "status": "ok",
     "timestamp": 1745853194609,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "VFZ2PYEN1KWz",
    "outputId": "6cc0edfd-36c8-409a-c3f1-46aaf4695603"
   },
   "outputs": [],
   "source": [
    "# === Create TrainingData Table by Joining Inspections and Restaurant Profiles ===\n",
    "# This cell creates a new BigQuery table called 'TrainingData'\n",
    "# by joining the inspection records (InspectionEvents) with restaurant profiles (RestaurantProfile_WithFlags)\n",
    "# on the 'place_id' key.\n",
    "# It **filters out** any inspections where place_id is NULL,\n",
    "# so only valid, matchable restaurants are included for machine learning model development.\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=\"hygiene-prediction-434\")\n",
    "\n",
    "# SQL query to create the joined TrainingData table\n",
    "query = \"\"\"\n",
    "CREATE OR REPLACE TABLE `hygiene-prediction-434.RestaurantModeling.TrainingData` AS\n",
    "SELECT\n",
    "  i.inspection_id,\n",
    "  i.place_id,\n",
    "  i.inspection_date,\n",
    "  i.inspection_type,\n",
    "  i.result,\n",
    "  i.violation_codes,\n",
    "  i.num_violations,\n",
    "  i.has_critical_violation,\n",
    "  i.risk,\n",
    "  r.dba_name,\n",
    "  r.rating,\n",
    "  r.price_level,\n",
    "  r.user_ratings_total,\n",
    "  r.business_status,\n",
    "  r.zip,\n",
    "  r.is_cafe,\n",
    "  r.is_bar,\n",
    "  r.is_bakery,\n",
    "  r.is_meal_takeaway,\n",
    "  r.is_meal_delivery,\n",
    "  r.is_night_club\n",
    "FROM\n",
    "  `hygiene-prediction-434.RestaurantModeling.InspectionEvents` AS i\n",
    "LEFT JOIN\n",
    "  `hygiene-prediction-434.RestaurantModeling.RestaurantProfile_WithFlags` AS r\n",
    "ON\n",
    "  i.place_id = r.place_id\n",
    "WHERE\n",
    "  i.place_id IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "query_job = client.query(query)\n",
    "query_job.result()\n",
    "\n",
    "print(\"âœ… TrainingData table created successfully (place_id NOT NULL).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 3113,
     "status": "ok",
     "timestamp": 1745853203871,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "yuo_y1BYJhVJ",
    "outputId": "bc87e320-de10-406c-919e-d3174998ea9c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# === Count Rows in TrainingData Table ===\n",
    "# Quick sanity check to verify the total number of rows after joining inspections and restaurant profiles.\n",
    "\n",
    "# === Display Schema of TrainingData Table ===\n",
    "# Prints the column names and types for the TrainingData table to understand available features for modeling.\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=\"hygiene-prediction-434\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT COUNT(*) AS total_rows\n",
    "FROM `hygiene-prediction-434.RestaurantModeling.TrainingData`\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(query).to_dataframe()\n",
    "df\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=\"hygiene-prediction-434\")\n",
    "\n",
    "# Fetch schema for the table\n",
    "table_ref = \"hygiene-prediction-434.RestaurantModeling.TrainingData\"\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# Print column names and types\n",
    "print()\n",
    "for field in table.schema:\n",
    "    print(f\"{field.name} ({field.field_type})\")\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=\"hygiene-prediction-434\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 3142,
     "status": "ok",
     "timestamp": 1745853214241,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "jqbQmBqbWvhk",
    "outputId": "80fa0db5-9d4d-421f-fea4-e9bfecbdbb38"
   },
   "outputs": [],
   "source": [
    "# ===  Load and Sort Inspections by Place and Date ===\n",
    "# This cell loads all inspection records from the TrainingData table,\n",
    "# and sorts them by 'place_id' and 'inspection_date' in ascending order.\n",
    "# Sorting is necessary to correctly build features based on previous inspections.\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=\"hygiene-prediction-434\")\n",
    "\n",
    "# SQL to fetch and sort inspections\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `hygiene-prediction-434.RestaurantModeling.TrainingData`\n",
    "ORDER BY\n",
    "  place_id,\n",
    "  inspection_date\n",
    "\"\"\"\n",
    "\n",
    "# Load into pandas dataframe\n",
    "df = client.query(query).to_dataframe()\n",
    "\n",
    "print(f\"âœ… Loaded {len(df)} inspection records, sorted by place_id and inspection_date.\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1745853219199,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "z95RTvsBKR5T",
    "outputId": "f578327f-28fd-4a6f-e7b8-643524fd46a5"
   },
   "outputs": [],
   "source": [
    "# === Create Fail Flag for Each Inspection ===\n",
    "# This cell creates a binary 'fail' flag from the 'result' column,\n",
    "# where 1 = failed inspection, 0 = passed or other outcomes.\n",
    "\n",
    "df['fail'] = df['result'].apply(lambda x: 1 if x == 'fail' else 0)\n",
    "\n",
    "print(\"âœ… Created 'fail' flag for each inspection.\")\n",
    "df[['place_id', 'inspection_date', 'result', 'fail']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1745853222633,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "8PcH10XrWL6F",
    "outputId": "711f7578-3b85-4b67-9bf9-14b840eb889d"
   },
   "outputs": [],
   "source": [
    "# === Assign Sequential Inspection Number per Restaurant ===\n",
    "# This cell assigns a sequential number ('inspection_number') to each inspection\n",
    "# within each 'place_id' group, ordered by inspection_date.\n",
    "# This will allow us to track past inspections for each restaurant.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assign a sequential inspection number within each restaurant (place_id)\n",
    "df['inspection_number'] = df.groupby('place_id').cumcount() + 1\n",
    "\n",
    "print(\"âœ… Assigned sequential inspection numbers.\")\n",
    "df[['place_id', 'inspection_date', 'inspection_number']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1745853228275,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "vZO7R04pWfkJ",
    "outputId": "79f35487-1b24-48ff-bb57-4937fac320f7"
   },
   "outputs": [],
   "source": [
    "# === Create Features Based on Prior Inspections ===\n",
    "# This cell creates basic historical features for each inspection,\n",
    "# using information from *previous* inspections (within the same place_id).\n",
    "# Features include:\n",
    "# - Total number of prior inspections\n",
    "# - Total number of prior critical violations\n",
    "# - Average number of violations per prior inspection\n",
    "# These features will help predict the outcome of the next inspection.\n",
    "\n",
    "# Create features:\n",
    "# Total prior inspections = inspection_number - 1\n",
    "df['total_prior_inspections'] = df['inspection_number'] - 1\n",
    "\n",
    "# Total prior critical violations (cumulative sum minus current row)\n",
    "df['prior_critical_violations'] = (\n",
    "    df.groupby('place_id')['has_critical_violation']\n",
    "    .cumsum()\n",
    "    .shift(1)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Total prior violations (cumulative sum of num_violations minus current row)\n",
    "df['prior_total_violations'] = (\n",
    "    df.groupby('place_id')['num_violations']\n",
    "    .cumsum()\n",
    "    .shift(1)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Average prior violations per inspection\n",
    "df['avg_prior_violations_per_inspection'] = (\n",
    "    df['prior_total_violations'] / df['total_prior_inspections']\n",
    ")\n",
    "df['avg_prior_violations_per_inspection'] = df['avg_prior_violations_per_inspection'].fillna(0)\n",
    "\n",
    "print(\"âœ… Created historical features based on prior inspections.\")\n",
    "df[['place_id', 'inspection_date', 'inspection_number', 'total_prior_inspections', 'prior_critical_violations', 'avg_prior_violations_per_inspection']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1745853233600,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "yyi8NLftNQA5",
    "outputId": "60c793f2-7b4b-4970-a47a-71d29f4349e0"
   },
   "outputs": [],
   "source": [
    "# === Create Fail Rate Feature (Past Failures Normalized by Inspections) ===\n",
    "# This cell creates a 'fail_rate' feature for each inspection,\n",
    "# defined as (prior failures) divided by (total prior inspections).\n",
    "\n",
    "# First, cumulative sum of past failures\n",
    "df['prior_failures'] = (\n",
    "    df.groupby('place_id')['fail']\n",
    "    .cumsum()\n",
    "    .shift(1)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Now compute fail rate\n",
    "df['fail_rate'] = df['prior_failures'] / df['total_prior_inspections']\n",
    "\n",
    "# Fill any NaN resulting from division by 0 with 0 (should only happen for bad data, but safe)\n",
    "df['fail_rate'] = df['fail_rate'].fillna(0)\n",
    "\n",
    "print(\"âœ… Created prior_failures and fail_rate features.\")\n",
    "df[['place_id', 'inspection_date', 'inspection_number', 'prior_failures', 'fail_rate']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1745853238939,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "arMsfltUWquQ",
    "outputId": "02adee47-52b0-4978-bb5d-4b0ce53884fd"
   },
   "outputs": [],
   "source": [
    "# === Drop First Inspections (No Prior History Available) ===\n",
    "# This cell removes inspections where 'inspection_number' == 1,\n",
    "# because these inspections have no historical data to use for feature engineering.\n",
    "# Only inspections with at least one prior inspection will be used for training.\n",
    "\n",
    "# Keep only inspections where inspection_number > 1\n",
    "df = df[df['inspection_number'] > 1].copy()\n",
    "\n",
    "print(f\"âœ… Remaining {len(df)} inspection records after dropping first inspections (no history).\")\n",
    "df[['place_id', 'inspection_date', 'inspection_number']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1745853243999,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "6vuuTrXvW3pL",
    "outputId": "24e880b4-19de-4d11-e15d-739a25ea4888"
   },
   "outputs": [],
   "source": [
    "# === Define Prediction Targets (Critical Violation and Pass/Fail) ===\n",
    "# This cell shifts relevant outcome columns upward within each place_id group,\n",
    "# so that the label (target) for each inspection is the outcome of the *next* inspection.\n",
    "# Targets:\n",
    "# - future_has_critical_violation: whether the next inspection had a critical violation\n",
    "# - future_fail: whether the next inspection resulted in 'fail'\n",
    "\n",
    "# Shift has_critical_violation to get the next inspection's critical violation result\n",
    "df['future_has_critical_violation'] = (\n",
    "    df.groupby('place_id')['has_critical_violation']\n",
    "    .shift(-1)\n",
    ")\n",
    "\n",
    "# Shift 'fail' flag to get the next inspection's failure\n",
    "df['future_fail'] = (\n",
    "    df.groupby('place_id')['fail']\n",
    "    .shift(-1)\n",
    ")\n",
    "\n",
    "# Drop rows where future target is missing (i.e., no next inspection exists)\n",
    "df = df[df['future_has_critical_violation'].notnull()].copy()\n",
    "\n",
    "print(f\"âœ… Remaining {len(df)} inspection records with future targets defined (critical violation and fail).\")\n",
    "df[['place_id', 'inspection_date', 'inspection_number', 'has_critical_violation', 'future_has_critical_violation', 'fail', 'future_fail']].head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1745854319486,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "ECwa5p6FLUud",
    "outputId": "5d781468-a9e7-4f6a-b38f-f021ebb15812"
   },
   "outputs": [],
   "source": [
    "# === Define Target Columns for Different Prediction Tasks ===\n",
    "# This cell defines separate target columns for different machine learning tasks:\n",
    "# - target_column_fail: whether the next inspection fails\n",
    "# - target_column_critical: whether the next inspection has a critical violation\n",
    "# Target selection will happen dynamically later during model training.\n",
    "\n",
    "# Define target columns\n",
    "target_column_fail = 'future_fail'\n",
    "target_column_critical = 'future_has_critical_violation'\n",
    "\n",
    "print(\"âœ… Target columns defined:\")\n",
    "print(f\"  - Fail Prediction: {target_column_fail}\")\n",
    "print(f\"  - Critical Violation Prediction: {target_column_critical}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1745854321647,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "kc4caGyaNhD1",
    "outputId": "ae3782ee-439c-420b-96f3-b477e7923707"
   },
   "outputs": [],
   "source": [
    "# === Correlation Analysis: Features vs Future Targets ===\n",
    "# This cell calculates the Pearson correlation between each feature\n",
    "# and both target labels: future_fail and future_has_critical_violation.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate correlation\n",
    "\n",
    "# Define features to check\n",
    "features_to_check = [\n",
    "    'total_prior_inspections',\n",
    "    'prior_critical_violations',\n",
    "    'prior_total_violations',\n",
    "    'avg_prior_violations_per_inspection',\n",
    "    'prior_failures',\n",
    "    'fail_rate',\n",
    "    'is_cafe',\n",
    "    'is_bar',\n",
    "    'is_bakery',\n",
    "    'is_meal_takeaway',\n",
    "    'is_meal_delivery',\n",
    "    'is_night_club',\n",
    "    'zip'\n",
    "]\n",
    "\n",
    "# Create empty list to collect results\n",
    "correlation_results = []\n",
    "\n",
    "# Loop through each feature\n",
    "for feature in features_to_check:\n",
    "    corr_fail = np.corrcoef(df[feature], df['future_fail'])[0, 1]\n",
    "    corr_critical = np.corrcoef(df[feature], df['future_has_critical_violation'])[0, 1]\n",
    "    correlation_results.append((feature, corr_fail, corr_critical))\n",
    "\n",
    "# Convert to dataframe\n",
    "correlation_df = pd.DataFrame(\n",
    "    correlation_results,\n",
    "    columns=['feature', 'correlation_with_future_fail', 'correlation_with_future_critical_violation']\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"âœ… Feature Correlation with Targets:\")\n",
    "display(correlation_df.sort_values(by='correlation_with_future_fail', ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1745854393521,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "Hd3QUdV2PIue",
    "outputId": "43a7eefb-f44c-429c-8b3d-7abff97d36be"
   },
   "outputs": [],
   "source": [
    "# === Define Feature Matrix (X) and Target Vector (y) ===\n",
    "\n",
    "# Pick the target you want\n",
    "target_column = target_column_fail\n",
    "#target_column =target_column_critical\n",
    "\n",
    "# Feature columns\n",
    "feature_columns = [\n",
    "    'total_prior_inspections',\n",
    "    'prior_critical_violations',\n",
    "    'prior_total_violations',\n",
    "    'avg_prior_violations_per_inspection',\n",
    "    'prior_failures',\n",
    "    'fail_rate',\n",
    "    'is_cafe',\n",
    "    'is_bar',\n",
    "    'is_bakery',\n",
    "    'is_meal_takeaway',\n",
    "    'is_meal_delivery',\n",
    "    'is_night_club',\n",
    "    'zip'\n",
    "]\n",
    "\n",
    "# Define X and y\n",
    "X = df[feature_columns]\n",
    "y = df[target_column]\n",
    "\n",
    "print(f\"âœ… Feature matrix X and target vector y defined.\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1745854409294,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "47JrzZc1OMVp",
    "outputId": "cd416447-d62b-4ee3-c94c-91f5ba49cdc9"
   },
   "outputs": [],
   "source": [
    "# === Train/Validation/Test Split ===   BASED ON THE CODE I GAVE YOU, give me a function that generates all the features\n",
    "# This cell splits the data into:\n",
    "# - Train set (70%)\n",
    "# - Validation set (15%)\n",
    "# - Test set (15%)\n",
    "# using random shuffling with a fixed random seed for reproducibility.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: Train (70%) vs Temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Validation (15%) vs Test (15%) from Temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=43, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"âœ… Split complete:\")\n",
    "print(f\"  - Training set: {X_train.shape[0]} rows\")\n",
    "print(f\"  - Validation set: {X_val.shape[0]} rows\")\n",
    "print(f\"  - Test set: {X_test.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1745854414923,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "sF4cbTRSPyX2",
    "outputId": "c2bdb651-ddf6-4126-c6a4-a3d8cacc7900"
   },
   "outputs": [],
   "source": [
    "# === Check Class Balance Before Modeling ===\n",
    "# This cell checks the distribution of the target labels (0 = pass, 1 = fail)\n",
    "# to understand how imbalanced the data is before training.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count labels in y\n",
    "label_counts = y.value_counts()\n",
    "\n",
    "print(\"âœ… Label counts:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Optional: Plot class distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Target Labels')\n",
    "plt.xlabel('critical violations')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 2061,
     "status": "ok",
     "timestamp": 1745854467468,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "4gOB8g0hgs1U",
    "outputId": "97aa7f20-dd6d-433f-9159-0039d97bd13f"
   },
   "outputs": [],
   "source": [
    "# === Train Logistic Regression, Random Forest, XGBoost and Ensemble Their Predictions ===\n",
    "# This cell trains three different models separately,\n",
    "# predicts probabilities, averages them, and makes final ensemble predictions.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Initialize all models\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=(len(y_train[y_train == 0]) / len(y_train[y_train == 1])),\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# 2. Train models\n",
    "logistic_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Get predicted probabilities (positive class only) on validation set\n",
    "logistic_probs = logistic_model.predict_proba(X_val)[:, 1]\n",
    "rf_probs = rf_model.predict_proba(X_val)[:, 1]\n",
    "xgb_probs = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 4. Average the probabilities\n",
    "ensemble_probs = (logistic_probs + rf_probs + xgb_probs) / 3\n",
    "\n",
    "# 5. Choose a threshold (start with 0.5)\n",
    "threshold = 0.5\n",
    "ensemble_preds = (ensemble_probs >= threshold).astype(int)\n",
    "\n",
    "# 6. Evaluate ensemble\n",
    "print(\"âœ… Ensemble Validation Performance (Threshold = 0.5):\")\n",
    "print(classification_report(y_val, ensemble_preds))\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, ensemble_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1745854671431,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "WWeAvRd0jhWq",
    "outputId": "91a17c41-ad42-459e-99fe-cbaee42d12e1"
   },
   "outputs": [],
   "source": [
    "# === Generate and Plot Risk Scores for Critical Events (Ensemble Model) ===\n",
    "# This cell calculates the probability of critical violation for each place\n",
    "# and plots a histogram of those probabilities.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Get ensemble probabilities\n",
    "# Assuming you've already computed:\n",
    "# logistic_probs, rf_probs, xgb_probs for validation set\n",
    "\n",
    "ensemble_probs = (logistic_probs + rf_probs + xgb_probs) / 3\n",
    "\n",
    "# 2. Plot histogram\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(ensemble_probs, bins=30, edgecolor='black')\n",
    "# plt.title('Distribution of Probabilities of Critical Violations')\n",
    "# plt.xlabel('Predicted Probability of Critical Violation')\n",
    "plt.title('Distribution of Probabilities of Failure')\n",
    "plt.xlabel('Predicted Probability of Failure')\n",
    "plt.ylabel('Number of Restaurants')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1745854677163,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "f4tLc2tNe-3-",
    "outputId": "82aa90d3-757e-466a-e8b0-f8ffc3569b5c"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import numpy as np  # <-- Make sure numpy is imported\n",
    "\n",
    "# 1. Set save directory\n",
    "models_dir = '/content/drive/MyDrive/msds434_project/models/'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# 2. Group three models into one dictionary\n",
    "ensemble_splitter = {\n",
    "    \"logistic_model\": logistic_model,\n",
    "    \"rf_model\": rf_model,\n",
    "    \"xgb_model\": xgb_model\n",
    "}\n",
    "\n",
    "# 3. Compute median threshold from predicted probabilities\n",
    "# (Make sure y_probs is already computed from your models)\n",
    "median_threshold = np.median(ensemble_probs)\n",
    "\n",
    "# 4. Save the ensemble splitter dictionary\n",
    "joblib.dump(ensemble_splitter, os.path.join(models_dir, 'ensemble_splitter_model.pkl'))\n",
    "\n",
    "# 5. Save the median threshold\n",
    "joblib.dump(median_threshold, os.path.join(models_dir, 'ensemble_splitter_median.pkl'))\n",
    "\n",
    "print(\"âœ… Saved ensemble splitter model dictionary.\")\n",
    "print(\"âœ… Saved ensemble splitter median threshold.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1745854681289,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "vAKrXdh7s7TI",
    "outputId": "86386f38-d845-4c99-d0c2-e9d62196c91c"
   },
   "outputs": [],
   "source": [
    "# === Median Split and Prepare Low-Risk and High-Risk Datasets ===\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1. Calculate median probability\n",
    "median_prob = np.median(ensemble_probs)\n",
    "print(f\"âœ… Median ensemble probability: {median_prob:.4f}\")\n",
    "\n",
    "# 2. Tag each restaurant as low-risk or high-risk\n",
    "low_risk_idx = ensemble_probs < median_prob\n",
    "high_risk_idx = ensemble_probs >= median_prob\n",
    "\n",
    "# 3. Split feature matrix and target vector\n",
    "X_low = X_val[low_risk_idx]\n",
    "y_low = y_val[low_risk_idx]\n",
    "\n",
    "X_high = X_val[high_risk_idx]\n",
    "y_high = y_val[high_risk_idx]\n",
    "\n",
    "print(f\"âœ… Low-risk group: {X_low.shape[0]} examples\")\n",
    "print(f\"âœ… High-risk group: {X_high.shape[0]} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1745854684372,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "NB0O7tnbtLP2",
    "outputId": "1c4c2511-d86b-4de7-ae80-13e8ece3a806"
   },
   "outputs": [],
   "source": [
    "# === Train LowRiskModel and HighRiskModel Separately ===\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Initialize models (you can use same settings for now)\n",
    "low_risk_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=(len(y_low[y_low == 0]) / len(y_low[y_low == 1])),\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "high_risk_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=(len(y_high[y_high == 0]) / len(y_high[y_high == 1])),\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# 2. Train the models\n",
    "low_risk_model.fit(X_low, y_low)\n",
    "high_risk_model.fit(X_high, y_high)\n",
    "\n",
    "print(\"âœ… LowRiskModel and HighRiskModel trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1745854688626,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "tvC1uXl6uJxz",
    "outputId": "1b53c3be-e798-49a3-ec2b-26ab03c597d9"
   },
   "outputs": [],
   "source": [
    "# === Separate Evaluation for LowRiskModel and HighRiskModel ===\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Predict for Low Risk Group\n",
    "low_preds = (low_risk_model.predict_proba(X_low)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "print(\"âœ… Low Risk Group Performance:\")\n",
    "print(classification_report(y_low, low_preds))\n",
    "print(\"Low Risk Group Confusion Matrix:\")\n",
    "print(confusion_matrix(y_low, low_preds))\n",
    "\n",
    "# 2. Predict for High Risk Group\n",
    "high_preds = (high_risk_model.predict_proba(X_high)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… High Risk Group Performance:\")\n",
    "print(classification_report(y_high, high_preds))\n",
    "print(\"High Risk Group Confusion Matrix:\")\n",
    "print(confusion_matrix(y_high, high_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1745854693557,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "U-gZiGsctTgi",
    "outputId": "e7e38e2b-5c04-40a4-fa10-5ded5985f2f8"
   },
   "outputs": [],
   "source": [
    "# === Score Validation Set Using the Two Specialized Models ===\n",
    "# This cell routes examples to the appropriate model based on median split\n",
    "# and combines predictions into one final output.\n",
    "\n",
    "# 1. Predict probabilities separately\n",
    "low_probs = low_risk_model.predict_proba(X_low)[:, 1]\n",
    "high_probs = high_risk_model.predict_proba(X_high)[:, 1]\n",
    "\n",
    "# 2. Combine probabilities into one aligned array\n",
    "# (initialize full-size array)\n",
    "final_probs = np.zeros_like(ensemble_probs)\n",
    "\n",
    "# Fill based on the split\n",
    "final_probs[low_risk_idx] = low_probs\n",
    "final_probs[high_risk_idx] = high_probs\n",
    "\n",
    "# 3. Set threshold (you can tune this later â€” start with 0.5)\n",
    "threshold = 0.5\n",
    "final_preds = (final_probs >= threshold).astype(int)\n",
    "\n",
    "# 4. Evaluate\n",
    "print(\"âœ… Combined Two-Model Validation Performance (Threshold = 0.5):\")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_val, final_preds))\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, final_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1745854703557,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "Qo7zSI04udHC",
    "outputId": "cb10ab65-1ca4-4be5-abea-71d4c4483fc6"
   },
   "outputs": [],
   "source": [
    "# === Final Test Set Evaluation: Route Test Data and Predict ===\n",
    "\n",
    "# 1. Get ensemble probabilities for test set\n",
    "# (You must have trained logistic_model, rf_model, xgb_model already)\n",
    "\n",
    "logistic_test_probs = logistic_model.predict_proba(X_test)[:, 1]\n",
    "rf_test_probs = rf_model.predict_proba(X_test)[:, 1]\n",
    "xgb_test_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "ensemble_test_probs = (logistic_test_probs + rf_test_probs + xgb_test_probs) / 3\n",
    "\n",
    "# 2. Median split based on test ensemble probabilities\n",
    "median_prob_test = np.median(ensemble_test_probs)\n",
    "print(f\"âœ… Test set median ensemble probability: {median_prob_test:.4f}\")\n",
    "\n",
    "low_risk_test_idx = ensemble_test_probs < median_prob_test\n",
    "high_risk_test_idx = ensemble_test_probs >= median_prob_test\n",
    "\n",
    "X_low_test = X_test[low_risk_test_idx]\n",
    "y_low_test = y_test[low_risk_test_idx]\n",
    "\n",
    "X_high_test = X_test[high_risk_test_idx]\n",
    "y_high_test = y_test[high_risk_test_idx]\n",
    "\n",
    "# 3. Predict with the correct model\n",
    "low_test_preds = (low_risk_model.predict_proba(X_low_test)[:, 1] >= 0.5).astype(int)\n",
    "high_test_preds = (high_risk_model.predict_proba(X_high_test)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# 4. Combine predictions\n",
    "final_test_preds = np.zeros_like(ensemble_test_probs, dtype=int)\n",
    "final_test_preds[low_risk_test_idx] = low_test_preds\n",
    "final_test_preds[high_risk_test_idx] = high_test_preds\n",
    "\n",
    "# 5. Evaluate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"âœ… Final Test Set Performance (Two-Model System):\")\n",
    "print(classification_report(y_test, final_test_preds))\n",
    "print(\"Test Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, final_test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 16215,
     "status": "ok",
     "timestamp": 1745854750039,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "4RcWh6hIxo2x",
    "outputId": "fce1edad-f8cb-4fc5-8521-2692d861e74b"
   },
   "outputs": [],
   "source": [
    "# === Full Clean Two-Model System with K-Fold Cross Validation Predicting failure ===\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for clean output\n",
    "\n",
    "# 1. Define features and target\n",
    "feature_columns = [\n",
    "    'total_prior_inspections',\n",
    "    'prior_critical_violations',\n",
    "    'prior_total_violations',\n",
    "    'avg_prior_violations_per_inspection',\n",
    "    'prior_failures',\n",
    "    'fail_rate',\n",
    "    'is_cafe',\n",
    "    'is_bar',\n",
    "    'is_bakery',\n",
    "    'is_meal_takeaway',\n",
    "    'is_meal_delivery',\n",
    "    'is_night_club',\n",
    "    'zip'\n",
    "]\n",
    "\n",
    "target_column = 'future_fail'\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df[target_column]\n",
    "\n",
    "# 2. Setup 5-Fold Cross Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "all_reports = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # 3. Train base models\n",
    "    logistic_model = LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced')\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, class_weight='balanced')\n",
    "    xgb_model = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "                              scale_pos_weight=(len(y_train[y_train == 0]) / len(y_train[y_train == 1])),\n",
    "                              random_state=42, verbosity=0, eval_metric='logloss')\n",
    "\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # 4. Ensemble probabilities\n",
    "    logistic_test_probs = logistic_model.predict_proba(X_test)[:, 1]\n",
    "    rf_test_probs = rf_model.predict_proba(X_test)[:, 1]\n",
    "    xgb_test_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    ensemble_test_probs = (logistic_test_probs + rf_test_probs + xgb_test_probs) / 3\n",
    "\n",
    "    # 5. Median split\n",
    "    median_prob_test = np.median(ensemble_test_probs)\n",
    "\n",
    "    low_risk_test_idx = ensemble_test_probs < median_prob_test\n",
    "    high_risk_test_idx = ensemble_test_probs >= median_prob_test\n",
    "\n",
    "    X_low = X_test[low_risk_test_idx]\n",
    "    y_low = y_test[low_risk_test_idx]\n",
    "\n",
    "    X_high = X_test[high_risk_test_idx]\n",
    "    y_high = y_test[high_risk_test_idx]\n",
    "\n",
    "    # 6. Train two specialized models\n",
    "    low_risk_model = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "                                   scale_pos_weight=(len(y_low[y_low == 0]) / len(y_low[y_low == 1])),\n",
    "                                   random_state=42, verbosity=0, eval_metric='logloss')\n",
    "\n",
    "    high_risk_model = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "                                    scale_pos_weight=(len(y_high[y_high == 0]) / len(y_high[y_high == 1])),\n",
    "                                    random_state=42, verbosity=0, eval_metric='logloss')\n",
    "\n",
    "    low_risk_model.fit(X_low, y_low)\n",
    "    high_risk_model.fit(X_high, y_high)\n",
    "\n",
    "    # 7. Predict using specialized models\n",
    "    low_preds = (low_risk_model.predict_proba(X_low)[:, 1] >= 0.5).astype(int)\n",
    "    high_preds = (high_risk_model.predict_proba(X_high)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "    final_test_preds = np.zeros_like(ensemble_test_probs, dtype=int)\n",
    "    final_test_preds[low_risk_test_idx] = low_preds\n",
    "    final_test_preds[high_risk_test_idx] = high_preds\n",
    "\n",
    "    # 8. Evaluate\n",
    "    report = classification_report(y_test, final_test_preds, output_dict=True)\n",
    "    confusion = confusion_matrix(y_test, final_test_preds)\n",
    "\n",
    "    print(f\"âœ… Fold {fold} Accuracy: {report['accuracy']:.4f}\")\n",
    "    print(f\"Fold {fold} Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "\n",
    "    all_reports.append(report)\n",
    "    fold += 1\n",
    "\n",
    "# 9. Average final metrics across folds\n",
    "avg_accuracy = np.mean([r['accuracy'] for r in all_reports])\n",
    "avg_recall_fail = np.mean([r['1.0']['recall'] for r in all_reports])\n",
    "avg_precision_fail = np.mean([r['1.0']['precision'] for r in all_reports])\n",
    "avg_f1_fail = np.mean([r['1.0']['f1-score'] for r in all_reports])\n",
    "\n",
    "print(\"\\n=== Final Cross-Validation Results ===\")\n",
    "print(f\"âœ… Average Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"âœ… Average Precision (Failures): {avg_precision_fail:.4f}\")\n",
    "print(f\"âœ… Average Recall (Failures): {avg_recall_fail:.4f}\")\n",
    "print(f\"âœ… Average F1-score (Failures): {avg_f1_fail:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1745854790814,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "rQvc9Hsqxow9"
   },
   "outputs": [],
   "source": [
    "# === Full Two-Model System with K-Fold CV for Critical Violations ===\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # 1. Define features and new target (CRITICAL VIOLATION prediction)\n",
    "# feature_columns = [\n",
    "#     'total_prior_inspections',\n",
    "#     'prior_critical_violations',\n",
    "#     'prior_total_violations',\n",
    "#     'avg_prior_violations_per_inspection',\n",
    "#     'prior_failures',\n",
    "#     'fail_rate',\n",
    "#     'is_cafe',\n",
    "#     'is_bar',\n",
    "#     'is_bakery',\n",
    "#     'is_meal_takeaway',\n",
    "#     'is_meal_delivery',\n",
    "#     'is_night_club',\n",
    "#     'zip'\n",
    "# ]\n",
    "\n",
    "# target_column = 'future_has_critical_violation'\n",
    "\n",
    "# X = df[feature_columns]\n",
    "# y = df[target_column]\n",
    "\n",
    "# # 2. Setup 5-Fold Cross Validation\n",
    "# kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# fold = 1\n",
    "# all_reports = []\n",
    "# balanced_accuracies = []\n",
    "\n",
    "# for train_idx, test_idx in kf.split(X, y):\n",
    "#     print(f\"\\n=== Fold {fold} ===\")\n",
    "\n",
    "#     # Split data\n",
    "#     X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "#     y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "#     # 3. Train base models\n",
    "#     logistic_model = LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced')\n",
    "#     rf_model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, class_weight='balanced')\n",
    "#     xgb_model = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "#                               scale_pos_weight=(len(y_train[y_train == 0]) / len(y_train[y_train == 1])),\n",
    "#                               random_state=42, verbosity=0, eval_metric='logloss')\n",
    "\n",
    "#     logistic_model.fit(X_train, y_train)\n",
    "#     rf_model.fit(X_train, y_train)\n",
    "#     xgb_model.fit(X_train, y_train)\n",
    "\n",
    "#     # 4. Ensemble probabilities\n",
    "#     logistic_test_probs = logistic_model.predict_proba(X_test)[:, 1]\n",
    "#     rf_test_probs = rf_model.predict_proba(X_test)[:, 1]\n",
    "#     xgb_test_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#     ensemble_test_probs = (logistic_test_probs + rf_test_probs + xgb_test_probs) / 3\n",
    "\n",
    "#     # 5. Median split\n",
    "#     median_prob_test = np.median(ensemble_test_probs)\n",
    "\n",
    "#     low_risk_test_idx = ensemble_test_probs < median_prob_test\n",
    "#     high_risk_test_idx = ensemble_test_probs >= median_prob_test\n",
    "\n",
    "#     X_low = X_test[low_risk_test_idx]\n",
    "#     y_low = y_test[low_risk_test_idx]\n",
    "\n",
    "#     X_high = X_test[high_risk_test_idx]\n",
    "#     y_high = y_test[high_risk_test_idx]\n",
    "\n",
    "#     # 6. Train two specialized models\n",
    "#     low_risk_model = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "#                                    scale_pos_weight=(len(y_low[y_low == 0]) / len(y_low[y_low == 1])),\n",
    "#                                    random_state=42, verbosity=0, eval_metric='logloss')\n",
    "\n",
    "#     high_risk_model = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "#                                     scale_pos_weight=(len(y_high[y_high == 0]) / len(y_high[y_high == 1])),\n",
    "#                                     random_state=42, verbosity=0, eval_metric='logloss')\n",
    "\n",
    "#     low_risk_model.fit(X_low, y_low)\n",
    "#     high_risk_model.fit(X_high, y_high)\n",
    "\n",
    "#     # 7. Predict using specialized models\n",
    "#     low_preds = (low_risk_model.predict_proba(X_low)[:, 1] >= 0.5).astype(int)\n",
    "#     high_preds = (high_risk_model.predict_proba(X_high)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "#     final_test_preds = np.zeros_like(ensemble_test_probs, dtype=int)\n",
    "#     final_test_preds[low_risk_test_idx] = low_preds\n",
    "#     final_test_preds[high_risk_test_idx] = high_preds\n",
    "\n",
    "#     # 8. Evaluate\n",
    "#     report = classification_report(y_test, final_test_preds, output_dict=True)\n",
    "#     confusion = confusion_matrix(y_test, final_test_preds)\n",
    "#     bal_acc = balanced_accuracy_score(y_test, final_test_preds)\n",
    "\n",
    "#     print(f\"âœ… Fold {fold} Accuracy: {report['accuracy']:.4f}\")\n",
    "#     print(f\"âœ… Fold {fold} Balanced Accuracy: {bal_acc:.4f}\")\n",
    "#     print(f\"Fold {fold} Confusion Matrix:\")\n",
    "#     print(confusion)\n",
    "\n",
    "#     all_reports.append(report)\n",
    "#     balanced_accuracies.append(bal_acc)\n",
    "#     fold += 1\n",
    "\n",
    "# # 9. Average final metrics across folds\n",
    "# avg_accuracy = np.mean([r['accuracy'] for r in all_reports])\n",
    "# avg_recall_fail = np.mean([r['1.0']['recall'] for r in all_reports])\n",
    "# avg_precision_fail = np.mean([r['1.0']['precision'] for r in all_reports])\n",
    "# avg_f1_fail = np.mean([r['1.0']['f1-score'] for r in all_reports])\n",
    "# avg_bal_acc = np.mean(balanced_accuracies)\n",
    "\n",
    "# print(\"\\n=== Final Cross-Validation Results ===\")\n",
    "# print(f\"âœ… Average Accuracy: {avg_accuracy:.4f}\")\n",
    "# print(f\"âœ… Average Precision (Critical Violation): {avg_precision_fail:.4f}\")\n",
    "# print(f\"âœ… Average Recall (Critical Violation): {avg_recall_fail:.4f}\")\n",
    "# print(f\"âœ… Average F1-score (Critical Violation): {avg_f1_fail:.4f}\")\n",
    "# print(f\"âœ… Average Balanced Accuracy: {avg_bal_acc:.4f}\")\n",
    "\n",
    "# # 10. Plot Balanced Accuracy Across Folds\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.plot(range(1,6), balanced_accuracies, marker='o')\n",
    "# plt.title('Balanced Accuracy Across Folds (Predicting Critical Violations)')\n",
    "# plt.xlabel('Fold')\n",
    "# plt.ylabel('Balanced Accuracy')\n",
    "# plt.ylim(0, 1)\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2787,
     "status": "ok",
     "timestamp": 1745854804148,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "cMIxoaZexotV",
    "outputId": "611c6695-ea5d-40e4-8150-61c5bffdd66c"
   },
   "outputs": [],
   "source": [
    "# === Mount Google Drive ===\n",
    "from google.colab import drive\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Mount drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 1. Set your save directory\n",
    "save_dir = '/content/drive/MyDrive/msds434_project/models/'\n",
    "\n",
    "# 2. Create directory if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 3. Save models\n",
    "joblib.dump(logistic_model, os.path.join(save_dir, 'logistic_model.pkl'))\n",
    "joblib.dump(rf_model, os.path.join(save_dir, 'rf_model.pkl'))\n",
    "joblib.dump(xgb_model, os.path.join(save_dir, 'xgb_model.pkl'))\n",
    "joblib.dump(low_risk_model, os.path.join(save_dir, 'low_risk_model.pkl'))\n",
    "joblib.dump(high_risk_model, os.path.join(save_dir, 'high_risk_model.pkl'))\n",
    "\n",
    "# 4. Save extra important objects (feature list and median threshold)\n",
    "joblib.dump(feature_columns, os.path.join(save_dir, 'feature_columns.pkl'))\n",
    "joblib.dump(median_prob_test, os.path.join(save_dir, 'median_threshold.pkl'))\n",
    "\n",
    "print(f\"âœ… All models and objects saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2636,
     "status": "ok",
     "timestamp": 1745856203878,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "LaTf3rL08-My",
    "outputId": "9c1a389a-2120-48d9-cb58-cea569709f83"
   },
   "outputs": [],
   "source": [
    "# === Save Models to GCS Bucket (ML-models) ===\n",
    "from google.cloud import storage\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 1. Set up your GCS bucket and client\n",
    "bucket_name = 'ml-prediction-models'  # <-- Your GCS bucket name\n",
    "gcs_prefix = 'models/'     # Optional folder inside bucket (keep things organized)\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# 2. Local temp directory for staging\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# 3. Define your model objects\n",
    "models_to_save = {\n",
    "    'logistic_model.pkl': logistic_model,\n",
    "    'rf_model.pkl': rf_model,\n",
    "    'xgb_model.pkl': xgb_model,\n",
    "    'low_risk_model.pkl': low_risk_model,\n",
    "    'high_risk_model.pkl': high_risk_model,\n",
    "    'feature_columns.pkl': feature_columns,\n",
    "    'median_threshold.pkl': median_prob_test\n",
    "}\n",
    "\n",
    "# 4. Save locally then upload each to GCS\n",
    "for filename, model_obj in models_to_save.items():\n",
    "    local_path = os.path.join(temp_dir, filename)\n",
    "\n",
    "    # Save to local temp file\n",
    "    joblib.dump(model_obj, local_path)\n",
    "\n",
    "    # Upload to GCS\n",
    "    blob = bucket.blob(f'{gcs_prefix}{filename}')\n",
    "    blob.upload_from_filename(local_path)\n",
    "    print(f\"âœ… Uploaded {filename} to gs://{bucket_name}/{gcs_prefix}{filename}\")\n",
    "\n",
    "print(\"âœ…âœ… All models and objects successfully saved to GCS bucket!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y-Ohxcz6xpq"
   },
   "source": [
    "# Full Automated Risk Generator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alxAtxP_5NXd"
   },
   "source": [
    "# Run Report Pipeline\n",
    "- Open save ML models\n",
    "- Select new restaurants to predict pass/fail next inspection\n",
    "- Build engineered features - prepare_features\n",
    "- Apply ML models and compute prob failure\n",
    "- Score high / medium/ low risk or Not Enough History\n",
    "- Run report generation, save to csv and to BigQuery table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_5nvvA5rwuP"
   },
   "source": [
    "# === Full Risk Scoring System Overview ===\n",
    "\n",
    "1. pull_random_place_ids(n)\n",
    "    - Pulls N random restaurant place_ids from RestaurantProfile\n",
    "    - (Optional) random seed control in Python\n",
    "\n",
    "2. run_full_risk_scoring_pipeline(inspector_id, month_year_tag, place_ids)\n",
    "    - Creates NextRestaurants table in BigQuery\n",
    "    - Creates NextInspectionEvents table in BigQuery\n",
    "    - Loads inspection events\n",
    "    - Engineers historical features:\n",
    "        - total_prior_inspections\n",
    "        - prior_critical_violations\n",
    "        - prior_total_violations\n",
    "        - avg_prior_violations_per_inspection\n",
    "        - prior_failures\n",
    "        - fail_rate\n",
    "    - Joins inspections with restaurant profiles\n",
    "    - Drops first inspections (inspection_number == 1)\n",
    "    - Calls generate_and_save_risk_report()\n",
    "\n",
    "3. generate_and_save_risk_report()\n",
    "    - Loads trained Logistic, RF, and XGB models\n",
    "    - Scores restaurants using ensemble average\n",
    "    - Routes to low-risk or high-risk models based on median threshold\n",
    "    - Assigns risk_zone: 'high', 'medium', 'low'\n",
    "    - Selects highest risk inspection per restaurant\n",
    "    - Calls augment_risk_report_with_unscored()\n",
    "    - Adds restaurants with no scorable history (\"Not enough history\")\n",
    "    - Sorts:\n",
    "        - Scored restaurants by predicted_failure_probability descending\n",
    "        - Unscored restaurants listed after scored\n",
    "    - Saves final report to:\n",
    "        - Local CSV\n",
    "        - BigQuery RiskReport table\n",
    "    - Displays final report\n",
    "\n",
    "# === Output Tables (BigQuery):\n",
    "- {inspector_id}_ScoringRun_{month_year_tag}_Restaurants\n",
    "- {inspector_id}_ScoringRun_{month_year_tag}_InspectionEvents\n",
    "- {inspector_id}_ScoringRun_{month_year_tag}_ModelInput\n",
    "- {inspector_id}_ScoringRun_{month_year_tag}_RiskReport\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "example run\n",
    "# Pull 30 random restaurants and run scoring for Inspector I4, April 2025\n",
    "pull_and_score_random(inspector_id='I8', month_year_tag='0425', n=60, top_n=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1745857803990,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "-_nTPfw1RFpz"
   },
   "outputs": [],
   "source": [
    "# === Final Full Risk Report Generator with Proper Sorting ===\n",
    "    # \"\"\"\n",
    "    # Full End-to-End Risk Scoring Pipeline for Restaurant Inspections.\n",
    "\n",
    "    # This function performs the complete risk scoring process:\n",
    "    # - Accepts a list of place_ids to be scored (either manually selected or pulled randomly)\n",
    "    # - Pulls restaurant profiles for these place_ids from BigQuery\n",
    "    # - Pulls all past inspection events for these restaurants from BigQuery\n",
    "    # - Engineers historical inspection features (fail rate, prior violations, etc.)\n",
    "    # - Merges inspection features with restaurant profile metadata\n",
    "    # - Drops first inspections (since no prior history exists)\n",
    "    # - Scores restaurants using pre-trained ensemble models (Logistic, RF, XGB)\n",
    "    # - Routes inspections based on ensemble probability threshold into low/high risk models\n",
    "    # - Generates a risk prioritization report\n",
    "    # - Augments the report to include all submitted restaurants, even if not scorable\n",
    "    # - Saves:\n",
    "    #     - Full feature dataset to BigQuery (ModelInput table)\n",
    "    #     - Final risk report to BigQuery (RiskReport table)\n",
    "    # - Displays the final sorted risk report\n",
    "\n",
    "    # Output tables and files are named dynamically:\n",
    "    # - Format: {inspector_id}_ScoringRun_{month_year_tag}_Restaurants\n",
    "    # - Example: I1_ScoringRun_0425_ModelInput, I1_ScoringRun_0425_RiskReport\n",
    "\n",
    "    # Parameters:\n",
    "    #     inspector_id (str): Inspector ID (e.g., 'I1', 'I2', etc.)\n",
    "    #     month_year_tag (str): Month and Year tag for the run (e.g., '0425' for April 2025)\n",
    "    #     place_ids (list): List of place_ids (str) representing restaurants to score\n",
    "    #     models_dir (str): Directory path where trained ML models are saved\n",
    "    #     output_dataset (str): BigQuery dataset where outputs will be saved\n",
    "\n",
    "    # Returns:\n",
    "    #     None\n",
    "    # \"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import io\n",
    "import random\n",
    "\n",
    "# === Full Final Risk Scoring Pipeline with Augmentation ===\n",
    "\n",
    "def run_full_risk_scoring_pipeline(inspector_id, month_year_tag, top_n, place_ids, models_dir='/content/drive/MyDrive/msds434_project/models/', output_dataset='hygiene-prediction-434.RestaurantModeling'):\n",
    "    \"\"\"\n",
    "    Full end-to-end risk scoring pipeline:\n",
    "    - Pull restaurants by given place_ids\n",
    "    - Pull inspection events\n",
    "    - Engineer historical features\n",
    "    - Merge with restaurant profiles\n",
    "    - Score failure risk using ensemble and specialized models\n",
    "    - Save model input (full_data) to BigQuery\n",
    "    - Save risk report to BigQuery\n",
    "\n",
    "    Parameters:\n",
    "        inspector_id (str): Inspector ID (e.g., 'I1')\n",
    "        month_year_tag (str): MonthYear tag (e.g., '0425')\n",
    "        place_ids (list): List of place_id strings to score\n",
    "        models_dir (str): Path to saved models\n",
    "        output_dataset (str): BigQuery dataset\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Create base run name\n",
    "    base_name = f\"{inspector_id}_ScoringRun_{month_year_tag}\"\n",
    "\n",
    "    client = bigquery.Client(project='hygiene-prediction-434')\n",
    "\n",
    "    # === Step 1: Create Restaurants Table ===\n",
    "    query_create_restaurants = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{output_dataset}.{base_name}_Restaurants` AS\n",
    "    SELECT\n",
    "      place_id,\n",
    "      dba_name,\n",
    "      address,\n",
    "      zip,\n",
    "      rating,\n",
    "      price_level,\n",
    "      user_ratings_total,\n",
    "      business_status,\n",
    "      types,\n",
    "      CASE WHEN 'cafe' IN UNNEST(types) THEN 1 ELSE 0 END AS is_cafe,\n",
    "      CASE WHEN 'bar' IN UNNEST(types) THEN 1 ELSE 0 END AS is_bar,\n",
    "      CASE WHEN 'bakery' IN UNNEST(types) THEN 1 ELSE 0 END AS is_bakery,\n",
    "      CASE WHEN 'meal_takeaway' IN UNNEST(types) THEN 1 ELSE 0 END AS is_meal_takeaway,\n",
    "      CASE WHEN 'meal_delivery' IN UNNEST(types) THEN 1 ELSE 0 END AS is_meal_delivery,\n",
    "      CASE WHEN 'night_club' IN UNNEST(types) THEN 1 ELSE 0 END AS is_night_club\n",
    "    FROM\n",
    "      `hygiene-prediction-434.RestaurantModeling.RestaurantProfile`\n",
    "    WHERE\n",
    "      place_id IN UNNEST(@place_ids)\n",
    "    \"\"\"\n",
    "\n",
    "    job_config_restaurants = bigquery.QueryJobConfig(\n",
    "        query_parameters=[bigquery.ArrayQueryParameter(\"place_ids\", \"STRING\", place_ids)]\n",
    "    )\n",
    "\n",
    "    client.query(query_create_restaurants, job_config=job_config_restaurants).result()\n",
    "    print(f\"âœ… Created {base_name}_Restaurants table with {len(place_ids)} restaurants.\")\n",
    "\n",
    "    # === Step 2: Create InspectionEvents Table ===\n",
    "    query_create_inspections = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{output_dataset}.{base_name}_InspectionEvents` AS\n",
    "    SELECT\n",
    "      i.inspection_id,\n",
    "      i.place_id,\n",
    "      i.inspection_date,\n",
    "      i.inspection_type,\n",
    "      i.result,\n",
    "      i.violation_codes,\n",
    "      i.num_violations,\n",
    "      i.has_critical_violation,\n",
    "      i.risk\n",
    "    FROM\n",
    "      `hygiene-prediction-434.RestaurantModeling.InspectionEvents` AS i\n",
    "    INNER JOIN\n",
    "      `{output_dataset}.{base_name}_Restaurants` AS r\n",
    "    ON\n",
    "      i.place_id = r.place_id\n",
    "    \"\"\"\n",
    "    client.query(query_create_inspections).result()\n",
    "    print(f\"âœ… Created {base_name}_InspectionEvents table.\")\n",
    "\n",
    "    # === Step 3: Load and Engineer Features ===\n",
    "    query_load_inspections = f\"\"\"\n",
    "    SELECT\n",
    "      inspection_id,\n",
    "      place_id,\n",
    "      inspection_date,\n",
    "      inspection_type,\n",
    "      result,\n",
    "      violation_codes,\n",
    "      num_violations,\n",
    "      has_critical_violation,\n",
    "      risk\n",
    "    FROM\n",
    "      `{output_dataset}.{base_name}_InspectionEvents`\n",
    "    ORDER BY\n",
    "      place_id,\n",
    "      inspection_date\n",
    "    \"\"\"\n",
    "    df_next_inspections = client.query(query_load_inspections).to_dataframe()\n",
    "    print(f\"âœ… Loaded {len(df_next_inspections)} rows from {base_name}_InspectionEvents.\")\n",
    "\n",
    "    # Feature Engineering\n",
    "    df_next_inspections['inspection_date'] = pd.to_datetime(df_next_inspections['inspection_date'])\n",
    "    df_next_inspections['fail'] = df_next_inspections['result'].apply(lambda x: 1 if x == 'fail' else 0)\n",
    "    df_next_inspections['inspection_number'] = df_next_inspections.groupby('place_id').cumcount() + 1\n",
    "    df_next_inspections['total_prior_inspections'] = df_next_inspections['inspection_number'] - 1\n",
    "    df_next_inspections['prior_critical_violations'] = df_next_inspections.groupby('place_id')['has_critical_violation'].cumsum().shift(1).fillna(0)\n",
    "    df_next_inspections['prior_total_violations'] = df_next_inspections.groupby('place_id')['num_violations'].cumsum().shift(1).fillna(0)\n",
    "    df_next_inspections['avg_prior_violations_per_inspection'] = (df_next_inspections['prior_total_violations'] / df_next_inspections['total_prior_inspections']).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    df_next_inspections['prior_failures'] = df_next_inspections.groupby('place_id')['fail'].cumsum().shift(1).fillna(0)\n",
    "    df_next_inspections['fail_rate'] = (df_next_inspections['prior_failures'] / df_next_inspections['total_prior_inspections']).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    print(f\"âœ… Engineered features successfully.\")\n",
    "\n",
    "    # === Step 4: Load Restaurants Metadata ===\n",
    "    query_load_restaurants = f\"\"\"\n",
    "    SELECT\n",
    "      place_id,\n",
    "      dba_name,\n",
    "      address,\n",
    "      zip,\n",
    "      rating,\n",
    "      price_level,\n",
    "      user_ratings_total,\n",
    "      business_status,\n",
    "      is_cafe,\n",
    "      is_bar,\n",
    "      is_bakery,\n",
    "      is_meal_takeaway,\n",
    "      is_meal_delivery,\n",
    "      is_night_club\n",
    "    FROM\n",
    "      `{output_dataset}.{base_name}_Restaurants`\n",
    "    \"\"\"\n",
    "    df_restaurants = client.query(query_load_restaurants).to_dataframe()\n",
    "    print(f\"âœ… Loaded {len(df_restaurants)} rows from {base_name}_Restaurants.\")\n",
    "\n",
    "    # Merge\n",
    "    full_data = df_next_inspections.merge(df_restaurants, how='left', on='place_id')\n",
    "    print(f\"âœ… Joined inspections with restaurant profiles. Final dataset has {full_data.shape[0]} rows.\")\n",
    "\n",
    "    # Drop first inspections\n",
    "    full_data = full_data[full_data['total_prior_inspections'] > 0].copy()\n",
    "    print(f\"âœ… Dropped first inspections. Remaining {full_data.shape[0]} rows.\")\n",
    "\n",
    "    # === Step 5: Score and Generate Risk Report ===\n",
    "    top_risk_report = generate_and_save_risk_report(\n",
    "      full_data,\n",
    "      models_dir=models_dir,\n",
    "      original_place_ids=place_ids,\n",
    "      df_restaurants_metadata=df_restaurants,\n",
    "      inspector_id=inspector_id,\n",
    "      month_year_tag=month_year_tag,\n",
    "      top_n=top_n\n",
    "    )\n",
    "\n",
    "\n",
    "    # === Step 6: Save full_data and risk_report to BigQuery ===\n",
    "    full_data_table = f\"{output_dataset}.{base_name}_ModelInput\"\n",
    "    client.load_table_from_dataframe(full_data, full_data_table).result()\n",
    "    print(f\"âœ… Saved full_data to BigQuery table: {full_data_table}\")\n",
    "\n",
    "    top_risk_report_table = f\"{output_dataset}.{base_name}_RiskReport\"\n",
    "    client.load_table_from_dataframe(top_risk_report, top_risk_report_table).result()\n",
    "    print(f\"âœ… Saved risk report to BigQuery table: {top_risk_report_table}\")\n",
    "\n",
    "    # === Step 7: Display Risk Report ===\n",
    "    display(top_risk_report)\n",
    "\n",
    "    print(\"âœ… Full risk scoring pipeline completed successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "#def generate_and_save_risk_report(new_data, models_dir, original_place_ids, df_restaurants_metadata, top_n=50, output_dir='/content/drive/MyDrive/msds434_project/outputs/'):\n",
    "def generate_and_save_risk_report(new_data, models_dir, original_place_ids, df_restaurants_metadata, inspector_id, month_year_tag, top_n=50, output_dir='/content/drive/MyDrive/msds434_project/outputs/'):\n",
    "\n",
    "    \"\"\"\n",
    "    Full scoring pipeline:\n",
    "    - Loads models and objects\n",
    "    - Scores new restaurants\n",
    "    - Ensures one row per restaurant (highest risk)\n",
    "    - Adds missing restaurants with 'Not enough history'\n",
    "    - Sorts scored first, unscored second\n",
    "    - Saves final risk report to Google Drive\n",
    "\n",
    "    Parameters:\n",
    "        new_data (DataFrame): New restaurant data with features matching 'feature_columns'\n",
    "        models_dir (str): Directory where models and objects are saved\n",
    "        original_place_ids (list): All place_ids that were supposed to be scored\n",
    "        df_restaurants_metadata (DataFrame): Basic metadata (dba_name, address, zip, place_id)\n",
    "        top_n (int): Number of top risky restaurants to keep\n",
    "        output_dir (str): Where to save the risk report CSV\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Final augmented risk report\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load models and objects\n",
    "\n",
    "    def load_joblib_from_gcs(bucket_name, blob_name):\n",
    "        \"\"\"Load a joblib object directly from a GCS bucket.\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        bytes_data = blob.download_as_bytes()\n",
    "        joblib_obj = joblib.load(io.BytesIO(bytes_data))\n",
    "\n",
    "        return joblib_obj\n",
    "\n",
    "\n",
    "    bucket_name = 'ml-prediction-models'\n",
    "\n",
    "    logistic_model = load_joblib_from_gcs(bucket_name, 'models/logistic_model.pkl')\n",
    "    rf_model = load_joblib_from_gcs(bucket_name, 'models/rf_model.pkl')\n",
    "    xgb_model = load_joblib_from_gcs(bucket_name, 'models/xgb_model.pkl')\n",
    "    low_risk_model = load_joblib_from_gcs(bucket_name, 'models/low_risk_model.pkl')\n",
    "    high_risk_model = load_joblib_from_gcs(bucket_name, 'models/high_risk_model.pkl')\n",
    "    feature_columns = load_joblib_from_gcs(bucket_name, 'models/feature_columns.pkl')\n",
    "    median_threshold = load_joblib_from_gcs(bucket_name, 'models/median_threshold.pkl')\n",
    "\n",
    "\n",
    "    # logistic_model = joblib.load(os.path.join(models_dir, 'logistic_model.pkl'))\n",
    "    # rf_model = joblib.load(os.path.join(models_dir, 'rf_model.pkl'))\n",
    "    # xgb_model = joblib.load(os.path.join(models_dir, 'xgb_model.pkl'))\n",
    "    # low_risk_model = joblib.load(os.path.join(models_dir, 'low_risk_model.pkl'))\n",
    "    # high_risk_model = joblib.load(os.path.join(models_dir, 'high_risk_model.pkl'))\n",
    "    # feature_columns = joblib.load(os.path.join(models_dir, 'feature_columns.pkl'))\n",
    "    # median_threshold = joblib.load(os.path.join(models_dir, 'median_threshold.pkl'))\n",
    "\n",
    "    # 2. Prepare input\n",
    "    X_new = new_data[feature_columns].copy()\n",
    "    X_new = np.nan_to_num(X_new, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # 3. Score with ensemble\n",
    "    logistic_probs = logistic_model.predict_proba(X_new)[:, 1]\n",
    "    rf_probs = rf_model.predict_proba(X_new)[:, 1]\n",
    "    xgb_probs = xgb_model.predict_proba(X_new)[:, 1]\n",
    "    ensemble_probs = (logistic_probs + rf_probs + xgb_probs) / 3\n",
    "\n",
    "    # 4. Routing based on median\n",
    "    low_risk_idx = ensemble_probs < median_threshold\n",
    "    high_risk_idx = ensemble_probs >= median_threshold\n",
    "\n",
    "    # 5. Specialized model predictions\n",
    "    low_probs = np.zeros_like(ensemble_probs)\n",
    "    high_probs = np.zeros_like(ensemble_probs)\n",
    "\n",
    "    if np.sum(low_risk_idx) > 0:\n",
    "        low_probs[low_risk_idx] = low_risk_model.predict_proba(X_new[low_risk_idx])[:, 1]\n",
    "    if np.sum(high_risk_idx) > 0:\n",
    "        high_probs[high_risk_idx] = high_risk_model.predict_proba(X_new[high_risk_idx])[:, 1]\n",
    "\n",
    "    # 6. Final risk probabilities\n",
    "    final_probs = np.zeros_like(ensemble_probs)\n",
    "    final_probs[low_risk_idx] = low_probs[low_risk_idx]\n",
    "    final_probs[high_risk_idx] = high_probs[high_risk_idx]\n",
    "\n",
    "    # 7. Assign risk zones\n",
    "    def assign_risk_zone(prob):\n",
    "        if prob >= 0.75:\n",
    "            return 'high'\n",
    "        elif prob >= 0.5:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "\n",
    "    risk_zones = [assign_risk_zone(prob) for prob in final_probs]\n",
    "\n",
    "    # 8. Build output DataFrame\n",
    "    output_scores = pd.DataFrame({\n",
    "        'predicted_failure_probability': final_probs,\n",
    "        'risk_zone': risk_zones\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    # 9. Add metadata if available\n",
    "    if 'dba_name' in new_data.columns or 'zip' in new_data.columns or 'address' in new_data.columns:\n",
    "        metadata_cols = [col for col in ['dba_name', 'address', 'zip', 'place_id'] if col in new_data.columns]\n",
    "        output_scores = new_data[metadata_cols].reset_index(drop=True).join(output_scores)\n",
    "\n",
    "    # 10. Scoring status\n",
    "    output_scores['scoring_status'] = output_scores['predicted_failure_probability'].apply(\n",
    "        lambda x: 'Scored' if pd.notnull(x) else 'Not enough history'\n",
    "    )\n",
    "\n",
    "    # 11. Highest risk per restaurant\n",
    "    output_scores = output_scores.sort_values(by='predicted_failure_probability', ascending=False)\n",
    "    output_scores = output_scores.drop_duplicates(subset=['place_id'])\n",
    "\n",
    "    # 12. Augment with missing restaurants\n",
    "    output_scores = augment_risk_report_with_unscored(output_scores, original_place_ids, df_restaurants_metadata)\n",
    "\n",
    "    # === FINAL SORT ===\n",
    "    # 13. Separate scored and unscored\n",
    "    scored_df = output_scores[output_scores['scoring_status'] == 'Scored'].copy()\n",
    "    unscored_df = output_scores[output_scores['scoring_status'] == 'Not enough history'].copy()\n",
    "\n",
    "    # 14. Sort scored restaurants by predicted_failure_probability descending\n",
    "    scored_df = scored_df.sort_values(by='predicted_failure_probability', ascending=False)\n",
    "\n",
    "    # 15. Concatenate\n",
    "    final_risk_report = pd.concat([scored_df, unscored_df], ignore_index=True).head(top_n)\n",
    "\n",
    "    # 16. Save to CSV\n",
    "    def upload_df_to_gcs(df, bucket_name, destination_blob_name):\n",
    "        \"\"\"Uploads a pandas DataFrame as CSV to a GCS bucket.\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "        csv_data = df.to_csv(index=False)\n",
    "        blob.upload_from_string(csv_data, content_type='text/csv')\n",
    "        print(f\"âœ… Uploaded risk report to: gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "    # Inside your main function (generate_and_save_risk_report)\n",
    "\n",
    "    gcs_filename = f\"{inspector_id}_{month_year_tag}_risk_report.csv\"\n",
    "\n",
    "\n",
    "    # Upload the final risk report DataFrame\n",
    "    upload_df_to_gcs(\n",
    "        final_risk_report,\n",
    "        bucket_name='restaurant-risk-reports',\n",
    "        destination_blob_name=gcs_filename\n",
    "    )\n",
    "\n",
    "    return final_risk_report\n",
    "\n",
    "\n",
    "\n",
    "# === Augment Risk Report to Include Unscored Restaurants ===\n",
    "\n",
    "def augment_risk_report_with_unscored(top_risk_report, original_place_ids, df_restaurants_metadata):\n",
    "    \"\"\"\n",
    "    Ensure every submitted restaurant appears in the final risk report,\n",
    "    even if no inspection history allowed scoring.\n",
    "\n",
    "    Parameters:\n",
    "        top_risk_report (DataFrame): Scored restaurants (subset)\n",
    "        original_place_ids (list): All place_ids submitted\n",
    "        df_restaurants_metadata (DataFrame): Metadata table (must have place_id, dba_name, address, zip)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Final augmented risk report with all restaurants\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Find missing place_ids\n",
    "    scored_place_ids = top_risk_report['place_id'].unique()\n",
    "    missing_place_ids = [pid for pid in original_place_ids if pid not in scored_place_ids]\n",
    "\n",
    "    print(f\"âœ… Found {len(missing_place_ids)} restaurants with no scorable inspections.\")\n",
    "\n",
    "    # 2. Build DataFrame for missing restaurants\n",
    "    missing_df = df_restaurants_metadata[df_restaurants_metadata['place_id'].isin(missing_place_ids)].copy()\n",
    "\n",
    "    missing_df['predicted_failure_probability'] = float('nan')\n",
    "    missing_df['risk_zone'] = 'Not Enough History to Score'\n",
    "    missing_df['scoring_status'] = 'Not enough history'\n",
    "\n",
    "    # 3. Select columns in right order\n",
    "    columns = ['dba_name', 'address', 'zip', 'place_id',\n",
    "               'predicted_failure_probability', 'risk_zone', 'scoring_status']\n",
    "\n",
    "    missing_df = missing_df[columns]\n",
    "\n",
    "    # 4. Align top_risk_report to same column order\n",
    "    top_risk_report = top_risk_report[columns]\n",
    "\n",
    "    # 5. Combine\n",
    "    final_risk_report = pd.concat([top_risk_report, missing_df], ignore_index=True)\n",
    "\n",
    "    # 6. Sort\n",
    "    final_risk_report = final_risk_report.sort_values(\n",
    "        by=['risk_zone', 'predicted_failure_probability'],\n",
    "        ascending=[True, False]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    print(f\"âœ… Final risk report completed: {final_risk_report.shape[0]} restaurants.\")\n",
    "\n",
    "    return final_risk_report\n",
    "\n",
    "\n",
    "# === Master Function to Pull Random Restaurants and Run Full Scoring ===\n",
    "\n",
    "def pull_and_score_random(inspector_id, month_year_tag, n=30, top_n=50, seed=42, models_dir='/content/drive/MyDrive/msds434_project/models/', output_dataset='hygiene-prediction-434.RestaurantModeling'):\n",
    "    \"\"\"\n",
    "    Pulls random place_ids and runs the full risk scoring pipeline in one call.\n",
    "\n",
    "    Parameters:\n",
    "        inspector_id (str): Inspector ID (e.g., 'I1')\n",
    "        month_year_tag (str): Month and year tag (e.g., '0425')\n",
    "        n (int): Number of random restaurants to pull\n",
    "        seed (int): Optional random seed for reproducibility\n",
    "        models_dir (str): Path to saved models\n",
    "        output_dataset (str): BigQuery dataset where results are saved\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull random place_ids\n",
    "    place_ids = pull_random_place_ids(n=n)\n",
    "\n",
    "    # Run full pipeline\n",
    "    run_full_risk_scoring_pipeline(\n",
    "        inspector_id=inspector_id,\n",
    "        month_year_tag=month_year_tag,\n",
    "        top_n=top_n,\n",
    "        place_ids=place_ids,\n",
    "        models_dir=models_dir,\n",
    "        output_dataset=output_dataset\n",
    "    )\n",
    "\n",
    "\n",
    "def pull_random_place_ids(n=30, seed=42):\n",
    "    \"\"\"\n",
    "    Pulls n random place_ids from the RestaurantProfile table.\n",
    "\n",
    "    Parameters:\n",
    "        n (int): Number of place_ids to pull\n",
    "        seed (int): Random seed for reproducibility (optional)\n",
    "\n",
    "    Returns:\n",
    "        list: List of random place_ids\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project='hygiene-prediction-434')\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT place_id\n",
    "    FROM `hygiene-prediction-434.RestaurantModeling.RestaurantProfile`\n",
    "    WHERE business_status = 'OPERATIONAL'\n",
    "    \"\"\"\n",
    "\n",
    "    df = client.query(query).to_dataframe()\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    place_ids = random.sample(df['place_id'].tolist(), n)\n",
    "\n",
    "    print(f\"âœ… Pulled {n} random place_ids.\")\n",
    "\n",
    "    return place_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 20527,
     "status": "ok",
     "timestamp": 1745857833080,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "3p3BCyf9iNJk",
    "outputId": "d9e60da5-e43d-4e5f-86a3-a2223cf36a92"
   },
   "outputs": [],
   "source": [
    "# Pull 30 random restaurants and run scoring for Inspector I4, April 2025\n",
    "pull_and_score_random(inspector_id='I6', month_year_tag='04-28-2025', n=60, top_n=50,seed=43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQRCYkpO9FmU"
   },
   "source": [
    "# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0el3_DJ5D3l"
   },
   "source": [
    "# Below Be Dragons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6842,
     "status": "ok",
     "timestamp": 1745858830183,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "T0duOcjSS3Bv",
    "outputId": "6f555bb3-2810-43df-af3b-a907a7a42b4f"
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 1. Mount Drive (if not already)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# 2. Set up\n",
    "local_folder = '/content/drive/MyDrive/msds434_project/'\n",
    "pattern = 'places_batch_checkpoint_start_offset_*.csv'\n",
    "gcs_bucket_name = 'restaurant-enrichment-data'\n",
    "\n",
    "# 3. Initialize GCS client\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(gcs_bucket_name)\n",
    "\n",
    "# 4. Find all matching files\n",
    "csv_files = glob.glob(os.path.join(local_folder, pattern))\n",
    "print(f\"âœ… Found {len(csv_files)} files to upload.\")\n",
    "\n",
    "# 5. Upload all files\n",
    "for local_file in csv_files:\n",
    "    # Get only the filename\n",
    "    filename = os.path.basename(local_file)\n",
    "    # Define where it will go in GCS (you can add a subfolder if you want)\n",
    "    blob = bucket.blob(filename)\n",
    "\n",
    "    # Upload\n",
    "    blob.upload_from_filename(local_file)\n",
    "    print(f\"âœ… Uploaded {filename} to gs://{gcs_bucket_name}/{filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRp_OJ5Z5CQs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1745713914026,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "t8RTp8CjkX8M",
    "outputId": "05ad081b-86d6-4637-99c2-657f16200fa3"
   },
   "outputs": [],
   "source": [
    "# === Tag High-Risk and Low-Risk Restaurants and Compare (Including ZIP) ===\n",
    "\n",
    "# 1. Create dataframe with ensemble probabilities and important features\n",
    "risk_df = pd.DataFrame({\n",
    "    'failure_probability': ensemble_probs,\n",
    "    'zip': X_val['zip'],\n",
    "    'total_prior_inspections': X_val['total_prior_inspections'],\n",
    "    'prior_critical_violations': X_val['prior_critical_violations'],\n",
    "    'prior_total_violations': X_val['prior_total_violations'],\n",
    "    'avg_prior_violations_per_inspection': X_val['avg_prior_violations_per_inspection'],\n",
    "    'prior_failures': X_val['prior_failures'],\n",
    "    'fail_rate': X_val['fail_rate'],\n",
    "    'is_cafe': X_val['is_cafe'],\n",
    "    'is_bar': X_val['is_bar'],\n",
    "    'is_bakery': X_val['is_bakery'],\n",
    "    'is_meal_takeaway': X_val['is_meal_takeaway'],\n",
    "    'is_meal_delivery': X_val['is_meal_delivery'],\n",
    "    'is_night_club': X_val['is_night_club']\n",
    "})\n",
    "\n",
    "# 2. Tag risk groups\n",
    "risk_df['risk_group'] = 'medium'  # default\n",
    "risk_df.loc[risk_df['failure_probability'] > 0.75, 'risk_group'] = 'high'\n",
    "risk_df.loc[risk_df['failure_probability'] < 0.4, 'risk_group'] = 'low'\n",
    "\n",
    "# 3. Filter only high and low risk\n",
    "compare_df = risk_df[risk_df['risk_group'].isin(['high', 'low'])]\n",
    "\n",
    "# 4. Group and summarize\n",
    "comparison_summary = compare_df.groupby('risk_group').mean()\n",
    "\n",
    "print(\"âœ… Comparison of High-Risk vs Low-Risk Restaurants:\")\n",
    "display(comparison_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1745713040050,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "T5WuT6ijkncg",
    "outputId": "d608262c-a89e-453e-bd98-2b5326cf1fef"
   },
   "outputs": [],
   "source": [
    "# === Plot Feature Differences Between High and Low Risk Groups ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Features you want to plot\n",
    "features_to_plot = [\n",
    "    'total_prior_inspections',\n",
    "    'prior_total_violations',\n",
    "    'avg_prior_violations_per_inspection',\n",
    "    'prior_failures',\n",
    "    'fail_rate'\n",
    "]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=len(features_to_plot), ncols=1, figsize=(8, len(features_to_plot)*3))\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    compare_df.boxplot(column=feature, by='risk_group', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Risk Group')\n",
    "    axes[idx].set_ylabel(feature)\n",
    "    axes[idx].set_xlabel('Risk Group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Comparisons: High vs Low Risk Groups', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1745713419009,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "ixRIqhwPoK5-",
    "outputId": "5e713d06-5b5a-475c-e339-14c5f6bb9624"
   },
   "outputs": [],
   "source": [
    "# === Full Risk Scoring System: Generate Risk Scores, Assign Zones, Visualize ===\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create Risk Scores Table\n",
    "risk_scores = pd.DataFrame({\n",
    "    'failure_probability': ensemble_probs,\n",
    "    'zip': X_val['zip']\n",
    "})\n",
    "risk_scores = risk_scores.sort_values(by='failure_probability', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 2. Define Risk Zones\n",
    "def assign_risk_zone(prob):\n",
    "    if prob >= 0.75:\n",
    "        return 'high'\n",
    "    elif prob >= 0.5:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'low'\n",
    "\n",
    "risk_scores['risk_zone'] = risk_scores['failure_probability'].apply(assign_risk_zone)\n",
    "\n",
    "# 3. Print Risk Zone Distribution\n",
    "print(\"âœ… Risk zone distribution:\")\n",
    "print(risk_scores['risk_zone'].value_counts())\n",
    "\n",
    "# 4. Visualize Risk Zones\n",
    "risk_scores['risk_zone'].value_counts().plot(kind='bar', color=['red', 'orange', 'green'])\n",
    "plt.title('Risk Zone Distribution')\n",
    "plt.xlabel('Risk Zone')\n",
    "plt.ylabel('Number of Restaurants')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# 5. Display top risky restaurants (optional)\n",
    "print(\"\\nâœ… Top 10 High-Risk Restaurants (by predicted failure probability):\")\n",
    "display(risk_scores[risk_scores['risk_zone'] == 'high'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsTviXv_RENv"
   },
   "outputs": [],
   "source": [
    "# === Create Days Since Last Inspection Feature ===\n",
    "# This cell calculates the number of days between each inspection and the previous inspection\n",
    "# for the same restaurant (place_id).\n",
    "\n",
    "# Ensure inspection_date is a datetime type\n",
    "df['inspection_date'] = pd.to_datetime(df['inspection_date'])\n",
    "\n",
    "# Sort again just to be 100% safe\n",
    "df = df.sort_values(['place_id', 'inspection_date']).reset_index(drop=True)\n",
    "\n",
    "# Calculate days since last inspection\n",
    "df['days_since_last_inspection'] = (\n",
    "    df.groupby('place_id')['inspection_date']\n",
    "    .diff()\n",
    "    .dt.days\n",
    ")\n",
    "\n",
    "# Fill NaNs (first inspection has no previous) with median gap\n",
    "median_gap = df['days_since_last_inspection'].median()\n",
    "df['days_since_last_inspection'] = df['days_since_last_inspection'].fillna(median_gap)\n",
    "\n",
    "print(\"âœ… Created days_since_last_inspection feature.\")\n",
    "df[['place_id', 'inspection_date', 'inspection_number', 'days_since_last_inspection']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQKcHoWRREHN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4yObYYTRD9X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OdeGsGHRDq8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1745510425910,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "-iz_3vPhhqT7",
    "outputId": "9377cdb8-1ae8-499e-ed8b-4110aa9ac231"
   },
   "outputs": [],
   "source": [
    "# Step 2: Shift features (t-1) and labels (t+1)\n",
    "df_seq[\"has_violation_38_t+1\"] = df_seq.groupby(\"dba_name\")[\"has_violation_38\"].shift(-1)\n",
    "\n",
    "# Create lagged (t-1) versions of all features you want to include\n",
    "lag_cols = [\n",
    "    \"zip\", \"facility_category\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\", \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "\n",
    "for col in lag_cols:\n",
    "    df_seq[f\"{col}_t-1\"] = df_seq.groupby(\"dba_name\")[col].shift(1)\n",
    "\n",
    "# Drop rows where t+1 or t-1 is missing\n",
    "df_seq = df_seq.dropna(subset=[\"has_violation_38_t+1\"] + [f\"{col}_t-1\" for col in lag_cols])\n",
    "\n",
    "# Convert the target to int (was float from shift)\n",
    "df_seq[\"has_violation_38_t+1\"] = df_seq[\"has_violation_38_t+1\"].astype(int)\n",
    "\n",
    "df_seq.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1745510494881,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "5B3wuXgUh8cj",
    "outputId": "571228d7-9b1e-465a-e822-99efe6ace1ce"
   },
   "outputs": [],
   "source": [
    "# Step 3: Define predictive features (current + lagged)\n",
    "feature_cols_current = [\n",
    "    \"zip\", \"facility_category\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\", \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "\n",
    "feature_cols_lagged = [f\"{col}_t-1\" for col in feature_cols_current]\n",
    "\n",
    "# Combine current and lagged features\n",
    "X_seq = pd.get_dummies(df_seq[feature_cols_current + feature_cols_lagged], drop_first=True)\n",
    "y_seq = df_seq[\"has_violation_38_t+1\"]\n",
    "\n",
    "X_seq.shape, y_seq.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75,
     "status": "ok",
     "timestamp": 1745510642550,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "1vZU-PM1ipMv",
    "outputId": "5a108d6a-a1ec-4d29-fb58-cd7478c9ca25"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First: train vs temp (70% train, 30% temp)\n",
    "X_train_seq, X_temp_seq, y_train_seq, y_temp_seq = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.3, stratify=y_seq, random_state=42\n",
    ")\n",
    "\n",
    "# Then: validation and test (15% each)\n",
    "X_val_seq, X_test_seq, y_val_seq, y_test_seq = train_test_split(\n",
    "    X_temp_seq, y_temp_seq, test_size=0.5, stratify=y_temp_seq, random_state=42\n",
    ")\n",
    "\n",
    "# Confirm sizes\n",
    "print(\"Train size:\", len(X_train_seq))\n",
    "print(\"Validation size:\", len(X_val_seq))\n",
    "print(\"Test size:\", len(X_test_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5869,
     "status": "ok",
     "timestamp": 1745510694624,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "ttbCXawNis0a",
    "outputId": "6355b26e-ae61-423c-d52f-2a25c609f179"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train Random Forest on temporal features\n",
    "rf_seq_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_seq_model.fit(X_train_seq, y_train_seq)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_seq_pred = rf_seq_model.predict(X_val_seq)\n",
    "\n",
    "print(\"Validation Performance â€” Predicting Violation 38 at Next Inspection (using two inspections):\")\n",
    "print(classification_report(y_val_seq, y_val_seq_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 3337,
     "status": "ok",
     "timestamp": 1745511094421,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "ar_9uelfjpoI",
    "outputId": "eb84302f-0420-435e-f555-6e1a3a72b955"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "  inspection_id,\n",
    "  inspection_date,\n",
    "  dba_name,\n",
    "  zip,\n",
    "  facility_category,\n",
    "  inspection_type,\n",
    "  risk,\n",
    "  violation_count,\n",
    "  has_violation_1,\n",
    "  has_violation_2,\n",
    "  has_violation_3,\n",
    "  has_violation_4,\n",
    "  has_violation_6,\n",
    "  has_violation_7,\n",
    "  has_violation_38,\n",
    "  has_supervision_violation,\n",
    "  has_employee_health_violation,\n",
    "  has_contamination_violation,\n",
    "  has_temp_control_violation,\n",
    "  has_food_source_violation,\n",
    "  has_equipment_violation\n",
    "FROM `hygiene-prediction-434.HygienePredictionColumn.CleanedInspectionColumn`\n",
    "WHERE has_violation_38 IS NOT NULL\n",
    "ORDER BY dba_name, inspection_date\n",
    "\"\"\"\n",
    "\n",
    "df_seq = client.query(query).to_dataframe()\n",
    "df_seq.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 32204,
     "status": "ok",
     "timestamp": 1745511185048,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "zeQ1z-aSjqko",
    "outputId": "0a04eeec-4c55-444f-f7ba-c9d7019a5974"
   },
   "outputs": [],
   "source": [
    "# Step 2: Create the \"any violation at t+1\" label\n",
    "\n",
    "# Identify all columns that start with 'has_violation_' and are not t-1 lag columns\n",
    "violation_cols = [\n",
    "    \"has_violation_1\",\n",
    "    \"has_violation_2\",\n",
    "    \"has_violation_3\",\n",
    "    \"has_violation_4\",\n",
    "    \"has_violation_6\",\n",
    "    \"has_violation_7\",\n",
    "    \"has_violation_38\",\n",
    "    \"has_supervision_violation\",\n",
    "    \"has_employee_health_violation\",\n",
    "    \"has_contamination_violation\",\n",
    "    \"has_temp_control_violation\",\n",
    "    \"has_food_source_violation\",\n",
    "    \"has_equipment_violation\"\n",
    "]\n",
    "\n",
    "# Shift all violation columns by -1 to get t+1 violations\n",
    "df_seq[\"has_any_violation_t+1\"] = (\n",
    "    df_seq.groupby(\"dba_name\")[violation_cols]\n",
    "    .transform(lambda x: x.shift(-1).fillna(0).astype(int))\n",
    "    .any(axis=1)\n",
    ").astype(int)\n",
    "\n",
    "# Show preview\n",
    "df_seq[[\"dba_name\", \"inspection_date\", \"has_any_violation_t+1\"] + violation_cols].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1745511212327,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "CtyKvoKWjrEY",
    "outputId": "f14d311e-ecc1-49c5-8d6d-2f86b3afda09"
   },
   "outputs": [],
   "source": [
    "# Step 3: Create tâ€“1 features for inspection context and violations\n",
    "lag_cols = [\n",
    "    \"zip\",\n",
    "    \"facility_category\",\n",
    "    \"inspection_type\",\n",
    "    \"risk\",\n",
    "    \"violation_count\",\n",
    "    \"has_violation_7\",\n",
    "    \"has_supervision_violation\",\n",
    "    \"has_violation_38\"\n",
    "]\n",
    "\n",
    "# Create lagged (tâ€“1) versions of selected features\n",
    "for col in lag_cols:\n",
    "    df_seq[f\"{col}_t-1\"] = df_seq.groupby(\"dba_name\")[col].shift(1)\n",
    "\n",
    "# Drop rows where t+1 label is missing or tâ€“1 features are missing\n",
    "df_seq = df_seq.dropna(subset=[\"has_any_violation_t+1\"] + [f\"{col}_t-1\" for col in lag_cols])\n",
    "\n",
    "# Convert label to int (in case it's float)\n",
    "df_seq[\"has_any_violation_t+1\"] = df_seq[\"has_any_violation_t+1\"].astype(int)\n",
    "\n",
    "df_seq.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1745511253569,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "kIcKsSZsjrYv",
    "outputId": "3ff38025-d4e5-472e-bb98-f01c8255f557"
   },
   "outputs": [],
   "source": [
    "# Step 4: Define full feature list (current + lagged)\n",
    "feature_cols_current = [\n",
    "    \"zip\",\n",
    "    \"facility_category\",\n",
    "    \"inspection_type\",\n",
    "    \"risk\",\n",
    "    \"violation_count\",\n",
    "    \"has_violation_7\",\n",
    "    \"has_supervision_violation\",\n",
    "    \"has_violation_38\"\n",
    "]\n",
    "\n",
    "feature_cols_lagged = [f\"{col}_t-1\" for col in feature_cols_current]\n",
    "\n",
    "# Build feature matrix (X) and target (y)\n",
    "X = pd.get_dummies(df_seq[feature_cols_current + feature_cols_lagged], drop_first=True)\n",
    "y = df_seq[\"has_any_violation_t+1\"]\n",
    "\n",
    "# Confirm shape\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1745511293041,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "EJHMc2h2jrqP",
    "outputId": "3d950462-d50a-46ff-9e62-b504517a6761"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train, validation, test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "# Confirm sizes\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11199,
     "status": "ok",
     "timestamp": 1745511353027,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "VnbYenmRjr4-",
    "outputId": "0f57373a-8040-4fcf-a163-34dbe7134dc7"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Validation Performance â€” Predicting Any Violation at Next Inspection:\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLSsqTBhjsLn"
   },
   "outputs": [],
   "source": [
    "# Ensure inspection_date is datetime\n",
    "df_seq[\"inspection_date\"] = pd.to_datetime(df_seq[\"inspection_date\"])\n",
    "\n",
    "# Group by facility and calculate days since last inspection\n",
    "df_seq[\"days_since_last\"] = (\n",
    "    df_seq.groupby(\"dba_name\")[\"inspection_date\"]\n",
    "    .diff()\n",
    "    .dt.days\n",
    ")\n",
    "\n",
    "# Drop rows where lagged features or labels are still missing\n",
    "df_seq = df_seq.dropna(subset=[\n",
    "    \"has_any_violation_t+1\",\n",
    "    \"days_since_last\"\n",
    "] + [f\"{col}_t-1\" for col in lag_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnP3xmdalvq4"
   },
   "outputs": [],
   "source": [
    "# Step 2: Compute change in violation count\n",
    "df_seq[\"violation_count_diff\"] = (\n",
    "    df_seq[\"violation_count\"] - df_seq[\"violation_count_t-1\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1745511549845,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "B6wf6_nhlwZP",
    "outputId": "7ecf817e-fc7c-4bf0-8dbf-2c6a6a5229a3"
   },
   "outputs": [],
   "source": [
    "# Step 3: Update the full feature list\n",
    "feature_cols_current = [\n",
    "    \"zip\",\n",
    "    \"facility_category\",\n",
    "    \"inspection_type\",\n",
    "    \"risk\",\n",
    "    \"violation_count\",\n",
    "    \"has_violation_7\",\n",
    "    \"has_supervision_violation\",\n",
    "    \"has_violation_38\"\n",
    "]\n",
    "\n",
    "feature_cols_lagged = [f\"{col}_t-1\" for col in feature_cols_current]\n",
    "\n",
    "# Include engineered features\n",
    "engineered_cols = [\"days_since_last\", \"violation_count_diff\"]\n",
    "\n",
    "# Build feature matrix and target\n",
    "X = pd.get_dummies(df_seq[feature_cols_current + feature_cols_lagged + engineered_cols], drop_first=True)\n",
    "y = df_seq[\"has_any_violation_t+1\"]\n",
    "\n",
    "# Confirm shape\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1745511593568,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "ejFkU-Aklwvl",
    "outputId": "090b9e94-b3a2-4401-a452-60913986ffd9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Redo the split using updated feature matrix\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6509,
     "status": "ok",
     "timestamp": 1745511627580,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "63ZgwUKNlxE6",
    "outputId": "0f00774a-0df8-4acc-d98d-5841580107f3"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Retrain the Random Forest with the improved features\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Validation Performance â€” Improved Model (Any Violation at Next Inspection):\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvq8FfpelxT_"
   },
   "outputs": [],
   "source": [
    "!pip install xgboost --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7537,
     "status": "ok",
     "timestamp": 1745512124198,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "_2Y5AIVFoO59",
    "outputId": "8a8e6c99-7986-4636-9769-53e1d4288544"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 2: Train XGBoost on the improved feature set\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),  # handle imbalance\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Validation Performance â€” XGBoost Model (Any Violation at Next Inspection):\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 2150,
     "status": "ok",
     "timestamp": 1745512409273,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "g9XmMb-ToPJv",
    "outputId": "48d2b015-4e88-42b8-eb04-8ef2310ed343"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "  inspection_id,\n",
    "  inspection_date,\n",
    "  dba_name,\n",
    "  zip,\n",
    "  facility_category,\n",
    "  inspection_type,\n",
    "  risk,\n",
    "  violation_count,\n",
    "  has_violation_1,\n",
    "  has_violation_2,\n",
    "  has_violation_3,\n",
    "  has_violation_4,\n",
    "  has_violation_6,\n",
    "  has_violation_7,\n",
    "  has_violation_38,\n",
    "  has_supervision_violation,\n",
    "  has_employee_health_violation,\n",
    "  has_contamination_violation,\n",
    "  has_temp_control_violation,\n",
    "  has_food_source_violation,\n",
    "  has_equipment_violation\n",
    "FROM `hygiene-prediction-434.HygienePredictionColumn.CleanedInspectionColumn`\n",
    "WHERE has_violation_38 IS NOT NULL\n",
    "ORDER BY dba_name, inspection_date\n",
    "\"\"\"\n",
    "\n",
    "df_seq = client.query(query).to_dataframe()\n",
    "df_seq[\"inspection_date\"] = pd.to_datetime(df_seq[\"inspection_date\"])\n",
    "df_seq.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuPCFUISoPbz"
   },
   "outputs": [],
   "source": [
    "# Step 2: Create t+1 label â€” \"any violation occurs at next inspection\"\n",
    "violation_cols = [\n",
    "    \"has_violation_1\",\n",
    "    \"has_violation_2\",\n",
    "    \"has_violation_3\",\n",
    "    \"has_violation_4\",\n",
    "    \"has_violation_6\",\n",
    "    \"has_violation_7\",\n",
    "    \"has_violation_38\",\n",
    "    \"has_supervision_violation\",\n",
    "    \"has_employee_health_violation\",\n",
    "    \"has_contamination_violation\",\n",
    "    \"has_temp_control_violation\",\n",
    "    \"has_food_source_violation\",\n",
    "    \"has_equipment_violation\"\n",
    "]\n",
    "\n",
    "df_seq[\"has_any_violation_t+1\"] = (\n",
    "    df_seq.groupby(\"dba_name\")[violation_cols]\n",
    "    .transform(lambda x: x.shift(-1).fillna(0).astype(int))\n",
    "    .any(axis=1)\n",
    ").astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1745512487236,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "an9sivgnoPtz",
    "outputId": "14de13b5-5ec7-4b2e-a1d1-9aa11dfaecca"
   },
   "outputs": [],
   "source": [
    "# Step 3: Define base features to lag\n",
    "base_features = [\n",
    "    \"zip\", \"facility_category\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\",\n",
    "    \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "\n",
    "# Create lagged features for t-1 and t-2\n",
    "for col in base_features:\n",
    "    df_seq[f\"{col}_t-1\"] = df_seq.groupby(\"dba_name\")[col].shift(1)\n",
    "    df_seq[f\"{col}_t-2\"] = df_seq.groupby(\"dba_name\")[col].shift(2)\n",
    "\n",
    "# Create days since last and since second-last inspections\n",
    "df_seq[\"days_since_t-1\"] = df_seq.groupby(\"dba_name\")[\"inspection_date\"].diff().dt.days\n",
    "df_seq[\"days_since_t-2\"] = df_seq.groupby(\"dba_name\")[\"inspection_date\"].diff(2).dt.days\n",
    "\n",
    "# Drop any rows missing t+1 label or lagged features\n",
    "lag_cols = [f\"{col}_t-1\" for col in base_features] + [f\"{col}_t-2\" for col in base_features]\n",
    "df_seq = df_seq.dropna(subset=[\"has_any_violation_t+1\", \"days_since_t-1\", \"days_since_t-2\"] + lag_cols)\n",
    "\n",
    "# Convert label to int (if needed)\n",
    "df_seq[\"has_any_violation_t+1\"] = df_seq[\"has_any_violation_t+1\"].astype(int)\n",
    "\n",
    "df_seq.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1745512514813,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "CQOmqPZ_oP-H",
    "outputId": "c8a183a9-b418-476e-9550-66cee0e23c96"
   },
   "outputs": [],
   "source": [
    "# Step 4: Define feature columns\n",
    "feature_cols_current = [\n",
    "    \"zip\", \"facility_category\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\",\n",
    "    \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "\n",
    "feature_cols_lagged_1 = [f\"{col}_t-1\" for col in feature_cols_current]\n",
    "feature_cols_lagged_2 = [f\"{col}_t-2\" for col in feature_cols_current]\n",
    "engineered_cols = [\"days_since_t-1\", \"days_since_t-2\"]\n",
    "\n",
    "# Combine all features\n",
    "X = pd.get_dummies(\n",
    "    df_seq[feature_cols_current + feature_cols_lagged_1 + feature_cols_lagged_2 + engineered_cols],\n",
    "    drop_first=True\n",
    ")\n",
    "y = df_seq[\"has_any_violation_t+1\"]\n",
    "\n",
    "# Confirm shape\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1745512544680,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "aDV99B9XoQOA",
    "outputId": "a9e03126-1bee-47a6-b084-eed657bd26e1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121605,
     "status": "ok",
     "timestamp": 1745512726748,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "GaXZf0cloQab",
    "outputId": "a9248e14-7b8d-4b85-e011-0a2613763890"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define base XGBoost model\n",
    "xgb_base = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Show best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1745512836568,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "J-yTTCQKoQ7y",
    "outputId": "5c612a37-1eb2-49ff-dc11-7cfcc2397f84"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Validation Performance â€” Tuned XGBoost Model:\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1745513012763,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "K3Q-GELsrpAL",
    "outputId": "de33195a-ca6d-435f-9c50-48690ac9d2d3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Final test set prediction\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance on test set\n",
    "print(\"Test Performance â€” Tuned XGBoost Model (Any Violation at Next Inspection):\")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1745513053318,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "2yQaqHKprpep",
    "outputId": "b2991f75-5691-4c49-d507-5fc46c7a9e07"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute and display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=[\"No Violation\", \"Any Violation\"]\n",
    ")\n",
    "\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix â€” Test Set (Tuned XGBoost)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1745513196549,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "vQJ1L6DGrptA",
    "outputId": "6e73ebad-2c7d-4900-d76d-78cb71500de7"
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter to restaurant facilities only\n",
    "df_rest = df_seq[df_seq[\"facility_category\"] == \"restaurant\"].copy()\n",
    "\n",
    "print(\"Filtered rows (restaurant only):\", len(df_rest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1745513290368,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "VkyOKx9mrqKO",
    "outputId": "d4bb5288-6b36-4d90-fe00-ae9b7303078f"
   },
   "outputs": [],
   "source": [
    "# Define the same feature structure\n",
    "feature_cols_current = [\n",
    "    \"zip\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\",\n",
    "    \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "\n",
    "feature_cols_lagged_1 = [f\"{col}_t-1\" for col in feature_cols_current]\n",
    "feature_cols_lagged_2 = [f\"{col}_t-2\" for col in feature_cols_current]\n",
    "engineered_cols = [\"days_since_t-1\", \"days_since_t-2\", \"violation_count_diff\"]\n",
    "\n",
    "# Recompute violation_count_diff after filtering\n",
    "df_rest[\"violation_count_diff\"] = (\n",
    "    df_rest[\"violation_count\"] - df_rest[\"violation_count_t-1\"]\n",
    ")\n",
    "\n",
    "# Build feature matrix for restaurants\n",
    "X_rest = pd.get_dummies(\n",
    "    df_rest[feature_cols_current + feature_cols_lagged_1 + feature_cols_lagged_2 + engineered_cols],\n",
    "    drop_first=True\n",
    ")\n",
    "y_rest = df_rest[\"has_any_violation_t+1\"]\n",
    "\n",
    "# Confirm shape\n",
    "X_rest.shape, y_rest.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1745513324910,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "HhZLL1wcrqWf",
    "outputId": "ed27e284-ab7e-4ea9-ccef-ceabef213ad4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split restaurant-only data\n",
    "X_rest_train, X_rest_temp, y_rest_train, y_rest_temp = train_test_split(\n",
    "    X_rest, y_rest, test_size=0.3, stratify=y_rest, random_state=42\n",
    ")\n",
    "\n",
    "X_rest_val, X_rest_test, y_rest_val, y_rest_test = train_test_split(\n",
    "    X_rest_temp, y_rest_temp, test_size=0.5, stratify=y_rest_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Restaurant Train size:\", len(X_rest_train))\n",
    "print(\"Restaurant Validation size:\", len(X_rest_val))\n",
    "print(\"Restaurant Test size:\", len(X_rest_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5021,
     "status": "ok",
     "timestamp": 1745513398310,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "obEH_P_irqhQ",
    "outputId": "50b728c6-61e7-407c-b6af-1bb23debd137"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train the restaurant-focused XGBoost model\n",
    "xgb_rest = xgb.XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=100,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=(y_rest_train == 0).sum() / (y_rest_train == 1).sum(),\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_rest.fit(X_rest_train, y_rest_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_rest_val_pred = xgb_rest.predict(X_rest_val)\n",
    "\n",
    "print(\"Validation Performance â€” XGBoost (Restaurants Only):\")\n",
    "print(classification_report(y_rest_val, y_rest_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1745513488478,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "UIK27A63rqq_",
    "outputId": "2b1ec8fb-7f36-474d-e383-8a105c5118b0"
   },
   "outputs": [],
   "source": [
    "# Predict on restaurant test set\n",
    "y_rest_test_pred = xgb_rest.predict(X_rest_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Test Performance â€” XGBoost (Restaurants Only):\")\n",
    "print(classification_report(y_rest_test, y_rest_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1745513528085,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "UBYg-icWrq0O",
    "outputId": "e316df50-97ce-4482-e5c5-6647df9ad264"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create and display confusion matrix\n",
    "cm_rest = confusion_matrix(y_rest_test, y_rest_test_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_rest,\n",
    "    display_labels=[\"No Violation\", \"Any Violation\"]\n",
    ")\n",
    "\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix â€” Test Set (XGBoost, Restaurants Only)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 148,
     "status": "ok",
     "timestamp": 1745513927927,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "ZygelaVTrq9P",
    "outputId": "030d7765-71e7-4238-c374-89a804b59a41"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the test set predictions\n",
    "results_df = pd.DataFrame({\n",
    "    \"y_true\": y_rest_test.values,\n",
    "    \"y_pred\": y_rest_test_pred,\n",
    "}, index=X_rest_test.index)\n",
    "\n",
    "# Label prediction result type\n",
    "conditions = [\n",
    "    (results_df.y_true == 1) & (results_df.y_pred == 1),\n",
    "    (results_df.y_true == 0) & (results_df.y_pred == 0),\n",
    "    (results_df.y_true == 1) & (results_df.y_pred == 0),\n",
    "    (results_df.y_true == 0) & (results_df.y_pred == 1),\n",
    "]\n",
    "labels = [\"TP\", \"TN\", \"FN\", \"FP\"]\n",
    "\n",
    "results_df[\"classification\"] = np.select(conditions, labels, default=\"Unknown\")\n",
    "\n",
    "\n",
    "# Merge with original restaurant inspection data\n",
    "df_misclass = df_rest.loc[results_df.index].copy()\n",
    "df_misclass[\"classification\"] = results_df[\"classification\"]\n",
    "\n",
    "# Preview results\n",
    "df_misclass[[\"dba_name\", \"inspection_date\", \"classification\", \"has_any_violation_t+1\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1745513989288,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "84G7_1R-rrF_",
    "outputId": "a220cc1e-45cb-475b-914e-4036f704b85e"
   },
   "outputs": [],
   "source": [
    "# Step 2: Group by classification label and compute feature averages\n",
    "group_summary = df_misclass.groupby(\"classification\")[\n",
    "    [\"violation_count\", \"days_since_t-1\", \"days_since_t-2\"]\n",
    "].mean().round(2)\n",
    "\n",
    "print(\"Average feature values by classification group:\")\n",
    "group_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "executionInfo": {
     "elapsed": 1387,
     "status": "ok",
     "timestamp": 1745514160272,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "YBN-q4sErrOu",
    "outputId": "671e1272-025f-4013-c40e-cb858945f98e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Barplot for each feature by classification group\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Violation count\n",
    "sns.barplot(\n",
    "    data=df_misclass,\n",
    "    x=\"classification\",\n",
    "    y=\"violation_count\",\n",
    "    ax=axes[0],\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "axes[0].set_title(\"Avg. Violation Count at Time t\")\n",
    "axes[0].set_ylabel(\"Avg. Violation Count\")\n",
    "axes[0].set_xlabel(\"Classification Outcome\")\n",
    "\n",
    "# Plot 2: Days since last inspection\n",
    "sns.barplot(\n",
    "    data=df_misclass,\n",
    "    x=\"classification\",\n",
    "    y=\"days_since_t-1\",\n",
    "    ax=axes[1],\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "axes[1].set_title(\"Avg. Days Since tâ€“1\")\n",
    "axes[1].set_ylabel(\"Days\")\n",
    "axes[1].set_xlabel(\"Classification Outcome\")\n",
    "\n",
    "# Plot 3: Days since second-last inspection\n",
    "sns.barplot(\n",
    "    data=df_misclass,\n",
    "    x=\"classification\",\n",
    "    y=\"days_since_t-2\",\n",
    "    ax=axes[2],\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "axes[2].set_title(\"Avg. Days Since tâ€“2\")\n",
    "axes[2].set_ylabel(\"Days\")\n",
    "axes[2].set_xlabel(\"Classification Outcome\")\n",
    "\n",
    "plt.suptitle(\"Comparison of Key Features by Classification Outcome\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1745517313292,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "Mjpo09mB7_8u",
    "outputId": "82dd086f-0aea-412f-c165-45b72ab5d5cc"
   },
   "outputs": [],
   "source": [
    "# Step 1: Extract month and season\n",
    "df_rest[\"month\"] = df_rest[\"inspection_date\"].dt.month\n",
    "\n",
    "# Map month to season (simplified US version)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"summer\"\n",
    "    else:\n",
    "        return \"fall\"\n",
    "\n",
    "df_rest[\"season\"] = df_rest[\"month\"].apply(get_season)\n",
    "\n",
    "df_rest[[\"inspection_date\", \"month\", \"season\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1745517347711,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "XcD6yg878Art",
    "outputId": "d84d9207-2c73-4ad5-d092-bb23da7ef030"
   },
   "outputs": [],
   "source": [
    "# Step 2: Flag complaint-driven inspections (case-insensitive)\n",
    "df_rest[\"is_complaint\"] = df_rest[\"inspection_type\"].str.contains(\"complaint\", case=False, na=False).astype(int)\n",
    "df_rest[\"is_reinspection\"] = df_rest[\"inspection_type\"].str.contains(\"re-inspection|reinspection\", case=False, na=False).astype(int)\n",
    "\n",
    "# Preview\n",
    "df_rest[[\"inspection_type\", \"is_complaint\", \"is_reinspection\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1745517383739,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "yO0bPJTq8BFf",
    "outputId": "23f9ada2-8fcf-42db-d9eb-92639eb9fc44"
   },
   "outputs": [],
   "source": [
    "# Step 3: Rebuild feature matrix with seasonality + complaint flags\n",
    "\n",
    "# Redefine feature columns\n",
    "feature_cols_current = [\n",
    "    \"zip\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\",\n",
    "    \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "\n",
    "feature_cols_lagged_1 = [f\"{col}_t-1\" for col in feature_cols_current]\n",
    "feature_cols_lagged_2 = [f\"{col}_t-2\" for col in feature_cols_current]\n",
    "engineered_cols = [\n",
    "    \"days_since_t-1\", \"days_since_t-2\", \"violation_count_diff\",\n",
    "    \"month\", \"season\", \"is_complaint\", \"is_reinspection\"\n",
    "]\n",
    "\n",
    "# Rebuild X and y\n",
    "X_rest = pd.get_dummies(\n",
    "    df_rest[feature_cols_current + feature_cols_lagged_1 + feature_cols_lagged_2 + engineered_cols],\n",
    "    drop_first=True\n",
    ")\n",
    "y_rest = df_rest[\"has_any_violation_t+1\"]\n",
    "\n",
    "# Confirm shape\n",
    "X_rest.shape, y_rest.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1745517418427,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "bkK1MS9s8BYA",
    "outputId": "e8d92840-c86c-475a-eff7-f072d0fb30aa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Re-split restaurant data with updated features\n",
    "X_rest_train, X_rest_temp, y_rest_train, y_rest_temp = train_test_split(\n",
    "    X_rest, y_rest, test_size=0.3, stratify=y_rest, random_state=42\n",
    ")\n",
    "\n",
    "X_rest_val, X_rest_test, y_rest_val, y_rest_test = train_test_split(\n",
    "    X_rest_temp, y_rest_temp, test_size=0.5, stratify=y_rest_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_rest_train))\n",
    "print(\"Validation size:\", len(X_rest_val))\n",
    "print(\"Test size:\", len(X_rest_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2446,
     "status": "ok",
     "timestamp": 1745517457308,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "hxNo5ZXt8Bk9",
    "outputId": "e9bc1817-12d2-4185-941b-2109df927da4"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Retrain XGBoost with additional features\n",
    "xgb_rest = xgb.XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=100,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=(y_rest_train == 0).sum() / (y_rest_train == 1).sum(),\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_rest.fit(X_rest_train, y_rest_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_rest_val_pred = xgb_rest.predict(X_rest_val)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Validation Performance â€” XGBoost (Restaurants with Seasonality + Complaint Flags):\")\n",
    "print(classification_report(y_rest_val, y_rest_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16048,
     "status": "ok",
     "timestamp": 1745517647707,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "R9Piv4Ig8Bxq",
    "outputId": "fdadbd62-b124-4830-c245-fc1ed75a7a91"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train logistic regression with class balancing\n",
    "log_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "log_model.fit(X_rest_train, y_rest_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_rest_val_pred_log = log_model.predict(X_rest_val)\n",
    "\n",
    "print(\"Validation Performance â€” Logistic Regression (Restaurants Only):\")\n",
    "print(classification_report(y_rest_val, y_rest_val_pred_log))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVhZNb0p8CBe"
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11743,
     "status": "ok",
     "timestamp": 1745517754410,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "fwJoEgpC8CVO",
    "outputId": "838298af-04b5-4acd-ab9c-d45d7fa420f4"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the LightGBM model\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "lgb_model.fit(X_rest_train, y_rest_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_rest_val_pred_lgb = lgb_model.predict(X_rest_val)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Validation Performance â€” LightGBM (Restaurants Only):\")\n",
    "print(classification_report(y_rest_val, y_rest_val_pred_lgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1745517952250,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "Yjg7ZSWO8Cl_",
    "outputId": "5870e240-f975-4aa1-8358-8c4c47849b23"
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute zip-level violation rates using all of df_rest (including non-test examples)\n",
    "zip_violation_rate = df_rest.groupby(\"zip\")[\"has_any_violation_t+1\"].mean()\n",
    "\n",
    "# Add as a new column to df_rest\n",
    "df_rest[\"zip_violation_rate\"] = df_rest[\"zip\"].map(zip_violation_rate)\n",
    "\n",
    "# Preview\n",
    "df_rest[[\"zip\", \"zip_violation_rate\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1745518001133,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "joF_QbJh-pO7",
    "outputId": "aea040f2-2250-4b9d-9084-26f6c7c2af06"
   },
   "outputs": [],
   "source": [
    "# Step 2: Rebuild X and y with zip_violation_rate included\n",
    "\n",
    "# Redefine all features\n",
    "feature_cols_current = [\n",
    "    \"zip\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\",\n",
    "    \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "\n",
    "feature_cols_lagged_1 = [f\"{col}_t-1\" for col in feature_cols_current]\n",
    "feature_cols_lagged_2 = [f\"{col}_t-2\" for col in feature_cols_current]\n",
    "engineered_cols = [\n",
    "    \"days_since_t-1\", \"days_since_t-2\", \"violation_count_diff\",\n",
    "    \"month\", \"season\", \"is_complaint\", \"is_reinspection\",\n",
    "    \"zip_violation_rate\"  # ðŸ‘ˆ NEW\n",
    "]\n",
    "\n",
    "# Create the updated feature matrix\n",
    "X_rest = pd.get_dummies(\n",
    "    df_rest[feature_cols_current + feature_cols_lagged_1 + feature_cols_lagged_2 + engineered_cols],\n",
    "    drop_first=True\n",
    ")\n",
    "y_rest = df_rest[\"has_any_violation_t+1\"]\n",
    "\n",
    "# Confirm shape\n",
    "X_rest.shape, y_rest.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1745518036771,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "NoeIcTGC-pst",
    "outputId": "8556013e-2f5c-429c-e556-679a51693623"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Re-split using updated feature matrix\n",
    "X_rest_train, X_rest_temp, y_rest_train, y_rest_temp = train_test_split(\n",
    "    X_rest, y_rest, test_size=0.3, stratify=y_rest, random_state=42\n",
    ")\n",
    "\n",
    "X_rest_val, X_rest_test, y_rest_val, y_rest_test = train_test_split(\n",
    "    X_rest_temp, y_rest_temp, test_size=0.5, stratify=y_rest_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_rest_train))\n",
    "print(\"Validation size:\", len(X_rest_val))\n",
    "print(\"Test size:\", len(X_rest_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2535,
     "status": "ok",
     "timestamp": 1745518066573,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "ervjliXa-qBw",
    "outputId": "fed60a2b-b11b-4ae1-f78a-08b7f851998d"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Retrain XGBoost with ZIP-level risk included\n",
    "xgb_rest = xgb.XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=100,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=(y_rest_train == 0).sum() / (y_rest_train == 1).sum(),\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_rest.fit(X_rest_train, y_rest_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_rest_val_pred = xgb_rest.predict(X_rest_val)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Validation Performance â€” XGBoost (with zip_violation_rate):\")\n",
    "print(classification_report(y_rest_val, y_rest_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1745518167020,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "nD_97k4w-qXZ",
    "outputId": "b4393bdc-0d88-4201-c94a-78bc485b4b08"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_rest_val, y_rest_val_pred)\n",
    "\n",
    "# Display it\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=[\"No Violation\", \"Any Violation\"]\n",
    ")\n",
    "\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix â€” Validation Set (XGBoost with zip_violation_rate)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgC35eK7XZiQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=\"hygiene-prediction-434\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr_F57xoXtGn"
   },
   "outputs": [],
   "source": [
    "# Google Cloud + BigQuery\n",
    "from google.colab import auth\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Tree-based models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Metrics & evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# MLP (you'll use this soon)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-OtDM6lW9ma"
   },
   "outputs": [],
   "source": [
    "# Load from BigQuery\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  inspection_id,\n",
    "  inspection_date,\n",
    "  dba_name,\n",
    "  zip,\n",
    "  facility_category,\n",
    "  inspection_type,\n",
    "  risk,\n",
    "  violation_count,\n",
    "  has_violation_1,\n",
    "  has_violation_2,\n",
    "  has_violation_3,\n",
    "  has_violation_4,\n",
    "  has_violation_6,\n",
    "  has_violation_7,\n",
    "  has_violation_38,\n",
    "  has_supervision_violation,\n",
    "  has_employee_health_violation,\n",
    "  has_contamination_violation,\n",
    "  has_temp_control_violation,\n",
    "  has_food_source_violation,\n",
    "  has_equipment_violation\n",
    "FROM `hygiene-prediction-434.HygienePredictionColumn.CleanedInspectionColumn`\n",
    "WHERE has_violation_38 IS NOT NULL\n",
    "ORDER BY dba_name, inspection_date\n",
    "\"\"\"\n",
    "\n",
    "df_seq = client.query(query).to_dataframe()\n",
    "df_seq[\"inspection_date\"] = pd.to_datetime(df_seq[\"inspection_date\"])\n",
    "\n",
    "# Create t+1 target\n",
    "violation_cols = [col for col in df_seq.columns if col.startswith(\"has_violation_\")]\n",
    "df_seq[\"has_any_violation_t+1\"] = (\n",
    "    df_seq.groupby(\"dba_name\")[violation_cols]\n",
    "    .transform(lambda x: x.shift(-1).fillna(0).astype(int))\n",
    "    .any(axis=1)\n",
    ").astype(int)\n",
    "\n",
    "# Create lagged features\n",
    "base_features = [\n",
    "    \"zip\", \"facility_category\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\", \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "\n",
    "for col in base_features:\n",
    "    df_seq[f\"{col}_t-1\"] = df_seq.groupby(\"dba_name\")[col].shift(1)\n",
    "    df_seq[f\"{col}_t-2\"] = df_seq.groupby(\"dba_name\")[col].shift(2)\n",
    "\n",
    "# Time between inspections\n",
    "df_seq[\"days_since_t-1\"] = df_seq.groupby(\"dba_name\")[\"inspection_date\"].diff().dt.days\n",
    "df_seq[\"days_since_t-2\"] = df_seq.groupby(\"dba_name\")[\"inspection_date\"].diff(2).dt.days\n",
    "\n",
    "# Add violation delta\n",
    "df_seq[\"violation_count_diff\"] = df_seq[\"violation_count\"] - df_seq[\"violation_count_t-1\"]\n",
    "\n",
    "# Add month and season\n",
    "df_seq[\"month\"] = df_seq[\"inspection_date\"].dt.month\n",
    "df_seq[\"season\"] = df_seq[\"month\"].map({12:\"winter\", 1:\"winter\", 2:\"winter\",\n",
    "                                        3:\"spring\", 4:\"spring\", 5:\"spring\",\n",
    "                                        6:\"summer\", 7:\"summer\", 8:\"summer\",\n",
    "                                        9:\"fall\", 10:\"fall\", 11:\"fall\"})\n",
    "\n",
    "# Complaint-based inspection flags\n",
    "df_seq[\"is_complaint\"] = df_seq[\"inspection_type\"].str.contains(\"complaint\", case=False, na=False).astype(int)\n",
    "df_seq[\"is_reinspection\"] = df_seq[\"inspection_type\"].str.contains(\"re[- ]?inspection\", case=False, na=False).astype(int)\n",
    "\n",
    "# Filter for restaurants and drop missing\n",
    "df_rest = df_seq[df_seq[\"facility_category\"] == \"restaurant\"].copy()\n",
    "lag_cols = [f\"{col}_t-1\" for col in base_features] + [f\"{col}_t-2\" for col in base_features]\n",
    "df_rest = df_rest.dropna(subset=[\"has_any_violation_t+1\", \"days_since_t-1\", \"days_since_t-2\", \"violation_count_diff\"] + lag_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1745541399035,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "dIrshAFgXLw7",
    "outputId": "f926f347-b537-44f5-c53e-fd63e2f2d79a"
   },
   "outputs": [],
   "source": [
    "# Compute ZIP-level violation rate and merge it\n",
    "zip_violation_rate = df_rest.groupby(\"zip\")[\"has_any_violation_t+1\"].mean()\n",
    "df_rest[\"zip_violation_rate\"] = df_rest[\"zip\"].map(zip_violation_rate)\n",
    "\n",
    "# Build X and y\n",
    "feature_cols_current = [\n",
    "    \"zip\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\", \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "feature_cols_lagged_1 = [f\"{col}_t-1\" for col in feature_cols_current]\n",
    "feature_cols_lagged_2 = [f\"{col}_t-2\" for col in feature_cols_current]\n",
    "engineered_cols = [\n",
    "    \"days_since_t-1\", \"days_since_t-2\", \"violation_count_diff\",\n",
    "    \"month\", \"season\", \"is_complaint\", \"is_reinspection\", \"zip_violation_rate\"\n",
    "]\n",
    "\n",
    "X_rest = pd.get_dummies(\n",
    "    df_rest[feature_cols_current + feature_cols_lagged_1 + feature_cols_lagged_2 + engineered_cols],\n",
    "    drop_first=True\n",
    ")\n",
    "y_rest = df_rest[\"has_any_violation_t+1\"]\n",
    "\n",
    "X_rest.shape, y_rest.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nrmd6J7PWYzP"
   },
   "outputs": [],
   "source": [
    "# Step 1: Split and scale data for MLP\n",
    "\n",
    "# Train/val/test split\n",
    "X_rest_train, X_rest_temp, y_rest_train, y_rest_temp = train_test_split(\n",
    "    X_rest, y_rest, test_size=0.3, stratify=y_rest, random_state=42\n",
    ")\n",
    "\n",
    "X_rest_val, X_rest_test, y_rest_val, y_rest_test = train_test_split(\n",
    "    X_rest_temp, y_rest_temp, test_size=0.5, stratify=y_rest_temp, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X_rest_train_scaled = scaler.fit_transform(X_rest_train)\n",
    "X_rest_val_scaled = scaler.transform(X_rest_val)\n",
    "X_rest_test_scaled = scaler.transform(X_rest_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpBSyDNo-qlF"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Normalize input features\n",
    "scaler = StandardScaler()\n",
    "X_rest_train_scaled = scaler.fit_transform(X_rest_train)\n",
    "X_rest_val_scaled = scaler.transform(X_rest_val)\n",
    "X_rest_test_scaled = scaler.transform(X_rest_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1745541561372,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "CVDVIYdb-q0l",
    "outputId": "96463166-517f-42e6-f616-1625684c8ef0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 2: Build the MLP model\n",
    "mlp_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_rest_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "mlp_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15154,
     "status": "ok",
     "timestamp": 1745541758360,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "4XJ6JyjMZSfC",
    "outputId": "9f9109ca-44da-410c-b44b-a24fabd99c85"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(\n",
    "    X_rest_train_scaled, y_rest_train,\n",
    "    validation_data=(X_rest_val_scaled, y_rest_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1  # ðŸ‘ˆ this shows live training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2016,
     "status": "ok",
     "timestamp": 1745541868392,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "Gi20URejZwmg",
    "outputId": "036c2098-79e3-4ea5-9315-f11ea6f1397d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_val_mlp_pred = (mlp_model.predict(X_rest_val_scaled) > 0.5).astype(int)\n",
    "print(\"Validation Performance â€” MLP:\")\n",
    "print(classification_report(y_rest_val, y_val_mlp_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 1567,
     "status": "ok",
     "timestamp": 1745542283138,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "vY0fcFRWbSJ_",
    "outputId": "30105e8a-dcb6-4f84-db95-9bd07df1720a"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM `hygiene-prediction-434.HygienePredictionRow.violation_code_sheet`\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "violation_code_df = client.query(query).to_dataframe()\n",
    "violation_code_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzINGeCF36RK"
   },
   "source": [
    "## ðŸ§ª Modeling Critical Violations by Facility Type (e.g., Schools)\n",
    "\n",
    "This section loads inspection data, identifies critical violations (codes 1â€“24), generates lagged and time-aware features, and trains an XGBoost model to predict whether a facility will receive a critical violation at its next inspection.\n",
    "\n",
    "Key components:\n",
    "- Authentication and data query from BigQuery\n",
    "- Definition of `has_critical_violation_t+1` target label\n",
    "- Feature engineering: lagged inspections, time gaps, zip-level risk\n",
    "- Facility-type filtering (e.g., `FACILITY_CATEGORY = \"school\"`)\n",
    "- Train/validation/test split and model training\n",
    "- Evaluation using classification report and 5-fold cross-validation with balanced accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "executionInfo": {
     "elapsed": 5412,
     "status": "ok",
     "timestamp": 1745549423385,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "GzsLflSmyE44",
    "outputId": "3e1df3e4-93f3-47cb-f1bf-4fa7de06aef1"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Authenticate (if needed)\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Reconnect to BigQuery\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=\"hygiene-prediction-434\")\n",
    "\n",
    "# Run query to get full inspection history with violation codes\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  inspection_id,\n",
    "  inspection_date,\n",
    "  dba_name,\n",
    "  zip,\n",
    "  facility_category,\n",
    "  inspection_type,\n",
    "  risk,\n",
    "  violation_count,\n",
    "  violation_codes,  -- needed for critical label\n",
    "  has_violation_1,\n",
    "  has_violation_2,\n",
    "  has_violation_3,\n",
    "  has_violation_4,\n",
    "  has_violation_6,\n",
    "  has_violation_7,\n",
    "  has_violation_38,\n",
    "  has_supervision_violation,\n",
    "  has_employee_health_violation,\n",
    "  has_contamination_violation,\n",
    "  has_temp_control_violation,\n",
    "  has_food_source_violation,\n",
    "  has_equipment_violation\n",
    "FROM `hygiene-prediction-434.HygienePredictionColumn.CleanedInspectionColumn`\n",
    "WHERE has_violation_38 IS NOT NULL\n",
    "ORDER BY dba_name, inspection_date\n",
    "\"\"\"\n",
    "\n",
    "# Load data\n",
    "df_seq = client.query(query).to_dataframe()\n",
    "df_seq[\"inspection_date\"] = pd.to_datetime(df_seq[\"inspection_date\"])\n",
    "\n",
    "df_seq[\"facility_category\"].value_counts().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Step 1: Identify critical violation codes (1â€“24)\n",
    "critical_codes = set(range(1, 25))  # Codes 1 through 24\n",
    "\n",
    "def extract_codes(v):\n",
    "    \"\"\"Extract integer codes from nested {'list': [{'element': X}]} structure\"\"\"\n",
    "    if isinstance(v, dict) and \"list\" in v:\n",
    "        return [int(item[\"element\"]) for item in v[\"list\"] if \"element\" in item]\n",
    "    return []\n",
    "\n",
    "def has_critical_violation(v):\n",
    "    \"\"\"Return 1 if any code is a critical violation\"\"\"\n",
    "    codes = extract_codes(v)\n",
    "    return int(any(code in critical_codes for code in codes))\n",
    "\n",
    "# Generate t+1 violation codes column\n",
    "df_seq[\"violation_codes_t+1\"] = df_seq.groupby(\"dba_name\")[\"violation_codes\"].shift(-1)\n",
    "\n",
    "# Create the binary critical label\n",
    "df_seq[\"has_critical_violation_t+1\"] = df_seq[\"violation_codes_t+1\"].apply(has_critical_violation)\n",
    "\n",
    "# Optional: check label balance\n",
    "df_seq[\"has_critical_violation_t+1\"].value_counts()\n",
    "\n",
    "# Step 2: Create lagged and temporal features\n",
    "\n",
    "# Lagged features: t-1 and t-2\n",
    "base_features = [\n",
    "    \"zip\", \"facility_category\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\", \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "\n",
    "for col in base_features:\n",
    "    df_seq[f\"{col}_t-1\"] = df_seq.groupby(\"dba_name\")[col].shift(1)\n",
    "    df_seq[f\"{col}_t-2\"] = df_seq.groupby(\"dba_name\")[col].shift(2)\n",
    "\n",
    "# Time deltas\n",
    "df_seq[\"days_since_t-1\"] = df_seq.groupby(\"dba_name\")[\"inspection_date\"].diff().dt.days\n",
    "df_seq[\"days_since_t-2\"] = df_seq.groupby(\"dba_name\")[\"inspection_date\"].diff(2).dt.days\n",
    "\n",
    "# Violation delta\n",
    "df_seq[\"violation_count_diff\"] = df_seq[\"violation_count\"] - df_seq[\"violation_count_t-1\"]\n",
    "\n",
    "# Season and inspection type flags\n",
    "df_seq[\"month\"] = df_seq[\"inspection_date\"].dt.month\n",
    "df_seq[\"season\"] = df_seq[\"month\"].map({12:\"winter\", 1:\"winter\", 2:\"winter\",\n",
    "                                        3:\"spring\", 4:\"spring\", 5:\"spring\",\n",
    "                                        6:\"summer\", 7:\"summer\", 8:\"summer\",\n",
    "                                        9:\"fall\", 10:\"fall\", 11:\"fall\"})\n",
    "\n",
    "df_seq[\"is_complaint\"] = df_seq[\"inspection_type\"].str.contains(\"complaint\", case=False, na=False).astype(int)\n",
    "df_seq[\"is_reinspection\"] = df_seq[\"inspection_type\"].str.contains(\"re[- ]?inspection\", case=False, na=False).astype(int)\n",
    "\n",
    "# Step 3: Filter based on facility category inspections and drop missing values\n",
    "FACILITY_CATEGORY = \"school\"\n",
    "df_rest = df_seq[df_seq[\"facility_category\"] == FACILITY_CATEGORY].copy()\n",
    "\n",
    "# Define required columns for modeling\n",
    "lag_cols = [f\"{col}_t-1\" for col in base_features] + [f\"{col}_t-2\" for col in base_features]\n",
    "required_features = lag_cols + [\"days_since_t-1\", \"days_since_t-2\", \"violation_count_diff\"]\n",
    "\n",
    "# Drop rows missing any required features or the new critical label\n",
    "df_rest = df_rest.dropna(subset=required_features + [\"has_critical_violation_t+1\"])\n",
    "\n",
    "# Step 4: Compute zip-level violation rate (still using has_any_violation_t+1 for context)\n",
    "zip_violation_rate = df_rest.groupby(\"zip\")[\"has_critical_violation_t+1\"].mean()\n",
    "df_rest[\"zip_violation_rate\"] = df_rest[\"zip\"].map(zip_violation_rate)\n",
    "\n",
    "# Define all features\n",
    "feature_cols_current = [\n",
    "    \"zip\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\", \"has_violation_7\",\n",
    "    \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "feature_cols_lagged_1 = [f\"{col}_t-1\" for col in feature_cols_current]\n",
    "feature_cols_lagged_2 = [f\"{col}_t-2\" for col in feature_cols_current]\n",
    "engineered_cols = [\n",
    "    \"days_since_t-1\", \"days_since_t-2\", \"violation_count_diff\",\n",
    "    \"month\", \"season\", \"is_complaint\", \"is_reinspection\",\n",
    "    \"zip_violation_rate\"\n",
    "]\n",
    "\n",
    "# Build feature matrix and target\n",
    "X_rest = pd.get_dummies(\n",
    "    df_rest[feature_cols_current + feature_cols_lagged_1 + feature_cols_lagged_2 + engineered_cols],\n",
    "    drop_first=True\n",
    ")\n",
    "y_rest = df_rest[\"has_critical_violation_t+1\"]\n",
    "\n",
    "# Confirm dimensions\n",
    "X_rest.shape, y_rest.shape\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train â†’ Temp â†’ Validation/Test split\n",
    "X_rest_train, X_rest_temp, y_rest_train, y_rest_temp = train_test_split(\n",
    "    X_rest, y_rest, test_size=0.3, stratify=y_rest, random_state=43\n",
    ")\n",
    "\n",
    "X_rest_val, X_rest_test, y_rest_val, y_rest_test = train_test_split(\n",
    "    X_rest_temp, y_rest_temp, test_size=0.5, stratify=y_rest_temp, random_state=43\n",
    ")\n",
    "\n",
    "# Confirm sizes\n",
    "print(\"Train size:\", len(X_rest_train))\n",
    "print(\"Validation size:\", len(X_rest_val))\n",
    "print(\"Test size:\", len(X_rest_test))\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train XGBoost for critical violation prediction\n",
    "xgb_critical = xgb.XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=100,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=(y_rest_train == 0).sum() / (y_rest_train == 1).sum(),\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_critical.fit(X_rest_train, y_rest_train)\n",
    "\n",
    "# Predict and evaluate on validation set\n",
    "y_rest_val_pred_crit = xgb_critical.predict(X_rest_val)\n",
    "\n",
    "print(\"\\nValidation Performance â€” XGBoost (Critical Violations Only):\")\n",
    "print(classification_report(y_rest_val, y_rest_val_pred_crit))\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Rebuild a fresh XGBoost model\n",
    "xgb_cv = xgb.XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=100,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=(y_rest == 0).sum() / (y_rest == 1).sum(),\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Evaluate with balanced accuracy using default threshold\n",
    "cv_scores = cross_val_score(\n",
    "    xgb_cv,\n",
    "    X_rest,\n",
    "    y_rest,\n",
    "    cv=kf,\n",
    "    scoring=make_scorer(balanced_accuracy_score)\n",
    ")\n",
    "\n",
    "# Print and plot\n",
    "print(\"Balanced Accuracy Scores:\", np.round(cv_scores, 3))\n",
    "print(f\"Mean Balanced Accuracy: {np.mean(cv_scores):.3f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(1, 6), cv_scores, color='skyblue')\n",
    "plt.axhline(y=np.mean(cv_scores), color='red', linestyle='--', label='Mean')\n",
    "plt.title(f\"Balanced Accuracy per Fold (5-Fold CV) â€” {FACILITY_CATEGORY.title()}\")\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Balanced Accuracy\")\n",
    "plt.ylim(0.5, 0.8)\n",
    "plt.xticks(range(1, 6))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIU3e75k_mIe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibGM0_qd_nHF"
   },
   "source": [
    "## ðŸ§® Partitioning Restaurants by Historical Violation Count\n",
    "\n",
    "In this section, we compute the total number of past violations for each restaurant and classify them into **low**, **medium**, and **high-risk tiers** using quantile-based binning. We then build separate prediction models for each group to explore whether historical performance improves model accuracy.\n",
    "\n",
    "**Steps included:**\n",
    "- Grouping inspection history by `dba_name`\n",
    "- Summing total `violation_count` per facility\n",
    "- Creating `violation_risk_tier` labels (`low`, `medium`, `high`)\n",
    "- Merging tiers into the full dataset\n",
    "- Training and evaluating models separately for each risk group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6179,
     "status": "ok",
     "timestamp": 1745591206261,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "Rg9b86Xbjlvn",
    "outputId": "05a6b3e7-89dd-4bfe-a005-646d1cfc0e35"
   },
   "outputs": [],
   "source": [
    "# Step 0: Authenticate and load full restaurant inspection data\n",
    "from IPython.display import display\n",
    "from google.colab import auth\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "auth.authenticate_user()\n",
    "client = bigquery.Client(project=\"hygiene-prediction-434\")\n",
    "\n",
    "# Pull relevant inspection fields\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  inspection_id,\n",
    "  inspection_date,\n",
    "  dba_name,\n",
    "  zip,\n",
    "  facility_category,\n",
    "  inspection_type,\n",
    "  risk,\n",
    "  violation_count,\n",
    "  violation_codes,\n",
    "  has_violation_1,\n",
    "  has_violation_2,\n",
    "  has_violation_3,\n",
    "  has_violation_4,\n",
    "  has_violation_6,\n",
    "  has_violation_7,\n",
    "  has_violation_38,\n",
    "  has_supervision_violation,\n",
    "  has_employee_health_violation,\n",
    "  has_contamination_violation,\n",
    "  has_temp_control_violation,\n",
    "  has_food_source_violation,\n",
    "  has_equipment_violation\n",
    "FROM `hygiene-prediction-434.HygienePredictionColumn.CleanedInspectionColumn`\n",
    "WHERE has_violation_38 IS NOT NULL\n",
    "ORDER BY dba_name, inspection_date\n",
    "\"\"\"\n",
    "\n",
    "# Load and format\n",
    "df_seq = client.query(query).to_dataframe()\n",
    "df_seq[\"inspection_date\"] = pd.to_datetime(df_seq[\"inspection_date\"])\n",
    "\n",
    "# Step 1: Identify critical violation codes (1â€“24)\n",
    "critical_codes = set(range(1, 25))\n",
    "\n",
    "def extract_codes(v):\n",
    "    if isinstance(v, dict) and \"list\" in v:\n",
    "        return [int(item[\"element\"]) for item in v[\"list\"] if \"element\" in item]\n",
    "    return []\n",
    "\n",
    "def has_critical_violation(v):\n",
    "    codes = extract_codes(v)\n",
    "    return int(any(code in critical_codes for code in codes))\n",
    "\n",
    "df_seq[\"violation_codes_t+1\"] = df_seq.groupby(\"dba_name\")[\"violation_codes\"].shift(-1)\n",
    "df_seq[\"has_critical_violation_t+1\"] = df_seq[\"violation_codes_t+1\"].apply(has_critical_violation)\n",
    "\n",
    "\n",
    "# Step 1: Identify critical violation codes (1â€“24)\n",
    "critical_codes = set(range(1, 25))\n",
    "\n",
    "def extract_codes(v):\n",
    "    if isinstance(v, dict) and \"list\" in v:\n",
    "        return [int(item[\"element\"]) for item in v[\"list\"] if \"element\" in item]\n",
    "    return []\n",
    "\n",
    "def has_critical_violation(v):\n",
    "    codes = extract_codes(v)\n",
    "    return int(any(code in critical_codes for code in codes))\n",
    "\n",
    "df_seq[\"violation_codes_t+1\"] = df_seq.groupby(\"dba_name\")[\"violation_codes\"].shift(-1)\n",
    "df_seq[\"has_critical_violation_t+1\"] = df_seq[\"violation_codes_t+1\"].apply(has_critical_violation)\n",
    "\n",
    "\n",
    "# Step 2: Total violation count per facility\n",
    "violation_totals = (\n",
    "    df_seq.groupby(\"dba_name\")[\"violation_count\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"violation_count\": \"total_violation_count\"})\n",
    ")\n",
    "\n",
    "# Classify into 3 risk tiers: low, medium, high\n",
    "violation_totals[\"violation_risk_tier\"] = pd.qcut(\n",
    "    violation_totals[\"total_violation_count\"],\n",
    "    q=3,\n",
    "    labels=[\"low\", \"medium\", \"high\"]\n",
    ")\n",
    "\n",
    "# Preview tier distribution\n",
    "display(violation_totals[\"violation_risk_tier\"].value_counts())\n",
    "\n",
    "# Step 3: Merge violation risk tiers into full dataset\n",
    "df_seq = df_seq.merge(violation_totals, on=\"dba_name\", how=\"left\")\n",
    "\n",
    "# Optional: Filter for restaurant category and one risk tier\n",
    "FACILITY_CATEGORY = \"restaurant\"\n",
    "#RISK_TIER = \"high\"  # change to \"low\", \"medium\", or \"high\"\n",
    "#RISK_TIER = \"medium\"  # change to \"low\", \"medium\", or \"high\"\n",
    "RISK_TIER = \"low\"  # change to \"low\", \"medium\", or \"high\"\n",
    "\n",
    "# Filter restaurant inspections in selected risk tier\n",
    "df_rest = df_seq[\n",
    "    (df_seq[\"facility_category\"] == FACILITY_CATEGORY) &\n",
    "    (df_seq[\"violation_risk_tier\"] == RISK_TIER)\n",
    "].copy()\n",
    "\n",
    "# Display size\n",
    "print(f\"{FACILITY_CATEGORY.title()}s in '{RISK_TIER}' risk tier: {len(df_rest)} rows\")\n",
    "df_rest[\"has_critical_violation_t+1\"].value_counts(normalize=True)\n",
    "\n",
    "\n",
    "# Step 4: Recreate lagged and engineered features\n",
    "\n",
    "base_features = [\n",
    "    \"zip\", \"facility_category\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\", \"has_violation_7\", \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "\n",
    "# Lag features (re-run even if they were previously created)\n",
    "for col in base_features:\n",
    "    df_rest[f\"{col}_t-1\"] = df_rest.groupby(\"dba_name\")[col].shift(1)\n",
    "    df_rest[f\"{col}_t-2\"] = df_rest.groupby(\"dba_name\")[col].shift(2)\n",
    "\n",
    "# Time deltas\n",
    "df_rest[\"days_since_t-1\"] = df_rest.groupby(\"dba_name\")[\"inspection_date\"].diff().dt.days\n",
    "df_rest[\"days_since_t-2\"] = df_rest.groupby(\"dba_name\")[\"inspection_date\"].diff(2).dt.days\n",
    "\n",
    "# Violation count change\n",
    "df_rest[\"violation_count_diff\"] = df_rest[\"violation_count\"] - df_rest[\"violation_count_t-1\"]\n",
    "\n",
    "# Seasonal and inspection flags\n",
    "df_rest[\"month\"] = df_rest[\"inspection_date\"].dt.month\n",
    "df_rest[\"season\"] = df_rest[\"month\"].map({12:\"winter\", 1:\"winter\", 2:\"winter\",\n",
    "                                          3:\"spring\", 4:\"spring\", 5:\"spring\",\n",
    "                                          6:\"summer\", 7:\"summer\", 8:\"summer\",\n",
    "                                          9:\"fall\", 10:\"fall\", 11:\"fall\"})\n",
    "\n",
    "df_rest[\"is_complaint\"] = df_rest[\"inspection_type\"].str.contains(\"complaint\", case=False, na=False).astype(int)\n",
    "df_rest[\"is_reinspection\"] = df_rest[\"inspection_type\"].str.contains(\"re[- ]?inspection\", case=False, na=False).astype(int)\n",
    "\n",
    "# Step 5: Drop missing rows for modeling\n",
    "lag_cols = [f\"{col}_t-1\" for col in base_features] + [f\"{col}_t-2\" for col in base_features]\n",
    "required_features = lag_cols + [\"days_since_t-1\", \"days_since_t-2\", \"violation_count_diff\"]\n",
    "\n",
    "df_rest = df_rest.dropna(subset=required_features + [\"has_critical_violation_t+1\"])\n",
    "\n",
    "# Rebuild ZIP-level critical violation rate for this tier only\n",
    "zip_violation_rate = df_rest.groupby(\"zip\")[\"has_critical_violation_t+1\"].mean()\n",
    "df_rest.loc[:, \"zip_violation_rate\"] = df_rest[\"zip\"].map(zip_violation_rate)\n",
    "\n",
    "\n",
    "# Define all features\n",
    "feature_cols_current = [\n",
    "    \"zip\", \"inspection_type\", \"risk\",\n",
    "    \"violation_count\", \"has_violation_7\",\n",
    "    \"has_supervision_violation\", \"has_violation_38\"\n",
    "]\n",
    "feature_cols_lagged_1 = [f\"{col}_t-1\" for col in feature_cols_current]\n",
    "feature_cols_lagged_2 = [f\"{col}_t-2\" for col in feature_cols_current]\n",
    "engineered_cols = [\n",
    "    \"days_since_t-1\", \"days_since_t-2\", \"violation_count_diff\",\n",
    "    \"month\", \"season\", \"is_complaint\", \"is_reinspection\",\n",
    "    \"zip_violation_rate\"\n",
    "]\n",
    "\n",
    "# Final feature matrix and target\n",
    "X_rest = pd.get_dummies(\n",
    "    df_rest[feature_cols_current + feature_cols_lagged_1 + feature_cols_lagged_2 + engineered_cols],\n",
    "    drop_first=True\n",
    ")\n",
    "y_rest = df_rest[\"has_critical_violation_t+1\"]\n",
    "\n",
    "# Confirm dimensions\n",
    "print(\"Feature matrix:\", X_rest.shape)\n",
    "print(\"Target distribution:\")\n",
    "display(y_rest.value_counts(normalize=True).rename(\"proportion\").to_frame())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train/validation/test split\n",
    "X_rest_train, X_rest_temp, y_rest_train, y_rest_temp = train_test_split(\n",
    "    X_rest, y_rest, test_size=0.3, stratify=y_rest, random_state=42\n",
    ")\n",
    "\n",
    "X_rest_val, X_rest_test, y_rest_val, y_rest_test = train_test_split(\n",
    "    X_rest_temp, y_rest_temp, test_size=0.5, stratify=y_rest_temp, random_state=42\n",
    ")\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_critical = xgb.XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=100,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=(y_rest_train == 0).sum() / (y_rest_train == 1).sum(),\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_critical.fit(X_rest_train, y_rest_train)\n",
    "\n",
    "# Predict and evaluate on validation set\n",
    "y_rest_val_pred = xgb_critical.predict(X_rest_val)\n",
    "\n",
    "print(\"Validation Performance â€” XGBoost (High-Risk Restaurants):\")\n",
    "print(classification_report(y_rest_val, y_rest_val_pred))\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 5-fold CV\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "xgb_cv = xgb.XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=100,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=(y_rest == 0).sum() / (y_rest == 1).sum(),\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Evaluate with balanced accuracy\n",
    "cv_scores = cross_val_score(\n",
    "    xgb_cv,\n",
    "    X_rest,\n",
    "    y_rest,\n",
    "    cv=kf,\n",
    "    scoring=make_scorer(balanced_accuracy_score)\n",
    ")\n",
    "\n",
    "# Print and plot\n",
    "print(\"Balanced Accuracy Scores:\", np.round(cv_scores, 3))\n",
    "print(f\"Mean Balanced Accuracy: {np.mean(cv_scores):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(1, 6), cv_scores, color='skyblue')\n",
    "plt.axhline(y=np.mean(cv_scores), color='red', linestyle='--', label='Mean')\n",
    "plt.title(\"Balanced Accuracy per Fold (5-Fold CV) â€” High-Risk Restaurants\")\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Balanced Accuracy\")\n",
    "plt.ylim(0.5, 0.8)\n",
    "plt.xticks(range(1, 6))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiLB7Mfc7iL9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmztIDM-7iYa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c88-QWAz_vwc"
   },
   "source": [
    "## ðŸŒ Enriching Restaurant Data with Yelp Attributes\n",
    "\n",
    "This section explores how to enhance our restaurant inspection dataset by retrieving external attributes from Yelp, using the restaurant's name and address. By matching facilities to Yelp business profiles, we aim to extract rich metadata that may improve model performance and segmentation strategies.\n",
    "\n",
    "**Yelp features we may incorporate:**\n",
    "- â­ Rating (1â€“5)\n",
    "- ðŸ’² Price tier\n",
    "- ðŸ½ï¸ Categories (e.g., \"Mexican\", \"Fast Food\", \"Sushi\")\n",
    "- ðŸ“ Neighborhood or full address match\n",
    "- ðŸ“· Photo count, review count, tags (optional)\n",
    "\n",
    "**Goals:**\n",
    "- Add external signals that reflect public perception, quality, and scale\n",
    "- Segment or cluster restaurants based on Yelp attributes\n",
    "- Train enhanced models with deeper understanding of restaurant characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1745591217943,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "0rfOxwXg_1c3",
    "outputId": "adbc3a87-766c-4593-e646-adc5651acf2c"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# ðŸ” Store your Yelp API Key here\n",
    "YELP_API_KEY = \"bQ-qx9Wt-NpgKfTQ-fEDgLwtcXQqeuRH9sCU1VaA5fMnXUAZgFJtcuqLNdban_geK1rLrP_525fHV-42SNUQx1AYkr1my6NnGq35tJu1x5-QDkD4mOu88P__lStUZXYx\"  # Replace with your key\n",
    "\n",
    "# Base URL for Yelp search\n",
    "YELP_SEARCH_URL = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "# Auth header\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {YELP_API_KEY}\"\n",
    "}\n",
    "\n",
    "\n",
    "def search_yelp(name, location, limit=1):\n",
    "    \"\"\"Search Yelp for a business by name and location (ZIP or full address)\"\"\"\n",
    "    params = {\n",
    "        \"term\": name,\n",
    "        \"location\": location,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    response = requests.get(YELP_SEARCH_URL, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} â€” {response.text}\")\n",
    "        return None\n",
    "\n",
    "def extract_yelp_fields(business_json):\n",
    "    \"\"\"Flatten Yelp business metadata from a search result\"\"\"\n",
    "    if not business_json or \"businesses\" not in business_json or not business_json[\"businesses\"]:\n",
    "        return None\n",
    "\n",
    "    b = business_json[\"businesses\"][0]\n",
    "    return {\n",
    "        \"name\": b.get(\"name\"),\n",
    "        \"yelp_rating\": b.get(\"rating\"),\n",
    "        \"yelp_price\": b.get(\"price\", None),\n",
    "        \"yelp_review_count\": b.get(\"review_count\"),\n",
    "        \"yelp_categories\": [cat[\"title\"] for cat in b.get(\"categories\", [])]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = search_yelp(\"McDonald's\", \"60614\")\n",
    "extracted = extract_yelp_fields(result)\n",
    "display(extracted)\n",
    "\n",
    "# Count unique restaurant names in the full dataset\n",
    "unique_restaurants = df_seq[\"dba_name\"].nunique()\n",
    "print(f\"Total unique restaurants: {unique_restaurants}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndmX8oE4Zp5R"
   },
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 2759,
     "status": "ok",
     "timestamp": 1745592036027,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "s_EYoNOqXaJh",
    "outputId": "cda18168-5ccf-427c-f9d4-ce449e0fdd92"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Explicitly set your project ID\n",
    "project_id = \"hygiene-prediction-434\"  # Replace with your actual project ID\n",
    "\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT dba_name, zip, address, state\n",
    "FROM `hygiene-prediction-434.HygienePredictionRow.CleanedInspectionRow`\n",
    "WHERE dba_name IS NOT NULL\n",
    "  AND zip IS NOT NULL\n",
    "  AND address IS NOT NULL\n",
    "  AND state IS NOT NULL\n",
    "ORDER BY RAND()\n",
    "LIMIT 500\n",
    "\"\"\"\n",
    "df_sample = client.query(query).to_dataframe()\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "executionInfo": {
     "elapsed": 79891,
     "status": "error",
     "timestamp": 1745594051395,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "1iqg4B4qZty1",
    "outputId": "7730b192-6020-4abf-ca9e-177bdec8833d"
   },
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "from itertools import chain\n",
    "import time\n",
    "\n",
    "# Yelp API setup\n",
    "YELP_API_KEY = \"qeczPXHn8NOxtheRkWPCD_tFdxyRfPuBTKMRcoxIxM_LxO5jF5X2Dh9Wr-TjsIKHg9i16PhCelg8AtgvQHDlm9ZQgRxGTBuaN90CPBESIRhYbDURuPripVZxna4LaHYx\"  # Replace with your key\n",
    "SEARCH_URL = \"https://api.yelp.com/v3/businesses/search\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {YELP_API_KEY}\"}\n",
    "\n",
    "def search_yelp(name, zip_code, max_retries=5):\n",
    "    params = {\"term\": name, \"location\": zip_code, \"limit\": 1}\n",
    "    for retry in range(max_retries):\n",
    "        response = requests.get(SEARCH_URL, headers=HEADERS, params=params)\n",
    "        if response.status_code == 200:\n",
    "            businesses = response.json().get(\"businesses\", [])\n",
    "            if businesses:\n",
    "                return businesses[0]\n",
    "            return None\n",
    "        elif response.status_code == 429:\n",
    "            wait_time = 2 ** retry\n",
    "            print(f\"Rate limit hit. Waiting {wait_time} seconds before retrying...\")\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} â€” {response.text}\")\n",
    "            return None\n",
    "    print(\"Max retries exceeded.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# BigQuery client\n",
    "client = bigquery.Client(project=\"hygiene-prediction-434\")\n",
    "\n",
    "# Fetch the data from BigQuery\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT dba_name, zip, address, state\n",
    "FROM `hygiene-prediction-434.HygienePredictionRow.CleanedInspectionRow`\n",
    "WHERE facility_type = 'restaurant'\n",
    "  AND dba_name IS NOT NULL\n",
    "  AND zip IS NOT NULL\n",
    "  AND address IS NOT NULL\n",
    "  AND state IS NOT NULL\n",
    "ORDER BY RAND()\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "df_sample = client.query(query).to_dataframe()\n",
    "\n",
    "# Verify the number of rows returned\n",
    "print(f\"Number of rows from BigQuery: {len(df_sample)}\")\n",
    "\n",
    "# Perform Yelp search for each row in the DataFrame with progress and rate limiting\n",
    "matches = []\n",
    "total_restaurants = len(df_sample)\n",
    "start_time = time.time()\n",
    "\n",
    "for i, (_, row) in enumerate(df_sample.iterrows(), 1):\n",
    "    result = search_yelp(row['dba_name'], row['zip'])\n",
    "    if result:\n",
    "        matches.append({\n",
    "            \"name\": row['dba_name'],\n",
    "            \"zip\": row['zip'],\n",
    "            \"address\": row['address'],\n",
    "            \"state\": row['state'],\n",
    "            \"yelp_id\": result.get(\"id\"),\n",
    "            \"yelp_rating\": result.get(\"rating\"),\n",
    "            \"yelp_price\": result.get(\"price\", \"N/A\"),\n",
    "            \"yelp_review_count\": result.get(\"review_count\"),\n",
    "            \"yelp_categories\": [cat['title'] for cat in result.get(\"categories\", [])]\n",
    "        })\n",
    "\n",
    "\n",
    "    # Print progress and estimate remaining time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_restaurant = elapsed_time / i\n",
    "    remaining_time = avg_time_per_restaurant * (total_restaurants - i)\n",
    "\n",
    "    print(f\"Processed {i}/{total_restaurants} restaurants. \"\n",
    "          f\"Estimated time remaining: {int(remaining_time)} seconds.\")\n",
    "\n",
    "    # Rate limiting: Pause for a short duration\n",
    "    time.sleep(2)  # Adjust as needed\n",
    "\n",
    "# Create a DataFrame from the matches\n",
    "matched_df = pd.DataFrame(matches)\n",
    "\n",
    "# Extract and analyze Yelp categories\n",
    "all_categories = list(chain.from_iterable(matched_df[\"categories\"]))\n",
    "unique_categories = sorted(set(all_categories))\n",
    "\n",
    "# Print the unique categories\n",
    "print(\"\\nUnique Yelp Categories:\")\n",
    "for category in unique_categories:\n",
    "    print(category)\n",
    "\n",
    "# Display the matched DataFrame\n",
    "matched_df.head()\n",
    "matched_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F62vPJXdtFPB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1745592859893,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "LdQ2hKMfbzz2",
    "outputId": "febc5cc4-337b-417d-a160-94cf0a2352f0"
   },
   "outputs": [],
   "source": [
    "matched_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1745593212154,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "evR11W-Aahbv",
    "outputId": "1ce228aa-a582-4276-bf8c-8986ee25f418"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "all_categories = list(chain.from_iterable(matched_df[\"categories\"]))\n",
    "category_counts = Counter(all_categories)\n",
    "\n",
    "# Convert to sorted DataFrame\n",
    "df_categories = (\n",
    "    pd.DataFrame(category_counts.items(), columns=[\"category\", \"count\"])\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_categories.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dXjdxONtGqj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "error",
     "timestamp": 1745593384320,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "032rTHyNd-k5",
    "outputId": "32f5163a-607f-4ec2-9a8f-d279dd177127"
   },
   "outputs": [],
   "source": [
    "price_counts = matched_df[\"price\"].value_counts().sort_index()\n",
    "print(price_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uV_H2l1tH19"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxI5QqO9nOUu"
   },
   "outputs": [],
   "source": [
    "# Fetch the data from BigQuery\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT dba_name, zip, address, state\n",
    "FROM `hygiene-prediction-434.HygienePredictionRow.CleanedInspectionRow`\n",
    "WHERE facility_type = 'restaurant'\n",
    "  AND dba_name IS NOT NULL\n",
    "  AND zip IS NOT NULL\n",
    "  AND address IS NOT NULL\n",
    "  AND state IS NOT NULL\n",
    "ORDER BY RAND()\n",
    "LIMIT 500\n",
    "\"\"\"\n",
    "df_sample = client.query(query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1745595782326,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "v-om5j6mnTSk",
    "outputId": "83fb8f74-adb5-4486-e1f9-75a8d4bffd5b"
   },
   "outputs": [],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTPNegwhtI4J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Arbr9by1tKZE"
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9rNEfFVtkTQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 18576,
     "status": "error",
     "timestamp": 1745598319756,
     "user": {
      "displayName": "Mark Lawley",
      "userId": "00680106046765312564"
     },
     "user_tz": 300
    },
    "id": "2Xv-tLjZt1H_",
    "outputId": "57f20a33-64c9-459f-8f43-440b19d208d3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOeR86TRuIpoH0VYK64dmgw",
   "provenance": [
    {
     "file_id": "1345Ol952rTVpn7QAYgbfmDRy8sFXFaM6",
     "timestamp": 1745598094761
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
